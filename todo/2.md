# Missing External Scripts

## scripts/business_rules.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Business Rules Processor
 * Handles complex business logic and decision making
 */

const fs = require("fs");
const path = require("path");

// Configuration from environment variables
const config = {
  rulesFile: process.env.CONFIG_RULES_FILE || "security_rules.json",
  alertThreshold: parseFloat(process.env.CONFIG_ALERT_THRESHOLD || "0.7"),
  businessHours: process.env.CONFIG_BUSINESS_HOURS || "09:00-17:00",
  escalationTime: parseInt(process.env.CONFIG_ESCALATION_TIME || "300"), // seconds
};

class BusinessRulesEngine {
  constructor() {
    this.rules = this.loadRules();
  }

  loadRules() {
    try {
      if (fs.existsSync(config.rulesFile)) {
        return JSON.parse(fs.readFileSync(config.rulesFile, "utf8"));
      }
    } catch (error) {
      console.error(`Error loading rules: ${error.message}`);
    }

    // Default rules
    return {
      person_detection: {
        business_hours: { priority: "medium", action: "log" },
        after_hours: { priority: "high", action: "alert" },
        restricted_zones: { priority: "critical", action: "immediate_alert" },
      },
      vehicle_detection: {
        parking_area: { priority: "low", action: "log" },
        restricted_area: { priority: "high", action: "alert" },
      },
      threat_assessment: {
        high_confidence: { threshold: 0.8, action: "escalate" },
        medium_confidence: { threshold: 0.6, action: "monitor" },
        low_confidence: { threshold: 0.3, action: "log" },
      },
    };
  }

  isBusinessHours() {
    const now = new Date();
    const currentTime = now.getHours() * 100 + now.getMinutes();
    const [start, end] = config.businessHours.split("-").map((time) => {
      const [hours, minutes] = time.split(":").map(Number);
      return hours * 100 + minutes;
    });
    return currentTime >= start && currentTime <= end;
  }

  processDetection(detection) {
    const { object_type, confidence, zone, position } = detection;
    const isAfterHours = !this.isBusinessHours();

    let decision = {
      original_detection: detection,
      business_priority: "low",
      recommended_action: "log",
      escalation_required: false,
      business_context: {},
      timestamp: new Date().toISOString(),
    };

    // Apply object-specific rules
    if (object_type === "person") {
      if (zone === "restricted" || zone === "entrance") {
        decision.business_priority = "critical";
        decision.recommended_action = "immediate_alert";
        decision.escalation_required = true;
      } else if (isAfterHours) {
        decision.business_priority = "high";
        decision.recommended_action = "alert";
      } else {
        decision.business_priority = "medium";
        decision.recommended_action = "monitor";
      }
    }

    if (object_type === "car") {
      if (zone === "parking") {
        decision.business_priority = "low";
        decision.recommended_action = "log";
      } else {
        decision.business_priority = "medium";
        decision.recommended_action = "monitor";
      }
    }

    // Apply confidence-based rules
    if (confidence > 0.8) {
      decision.business_priority = this.raisePriority(
        decision.business_priority,
      );
    }

    // Add business context
    decision.business_context = {
      is_business_hours: !isAfterHours,
      zone_risk_level: this.getZoneRiskLevel(zone),
      time_based_modifier: isAfterHours ? 1.5 : 1.0,
      requires_immediate_response: decision.business_priority === "critical",
    };

    return decision;
  }

  raisePriority(currentPriority) {
    const priorities = ["low", "medium", "high", "critical"];
    const currentIndex = priorities.indexOf(currentPriority);
    return priorities[Math.min(currentIndex + 1, priorities.length - 1)];
  }

  getZoneRiskLevel(zone) {
    const riskLevels = {
      entrance: "high",
      restricted: "critical",
      parking: "low",
      perimeter: "medium",
      garden: "low",
    };
    return riskLevels[zone] || "medium";
  }

  processAggregatedEvents(events) {
    const decisions = events.map((event) => this.processDetection(event));

    // Analyze patterns
    const patterns = this.analyzePatterns(decisions);

    return {
      individual_decisions: decisions,
      pattern_analysis: patterns,
      overall_threat_level: this.calculateOverallThreat(decisions),
      recommended_actions: this.recommendActions(decisions, patterns),
    };
  }

  analyzePatterns(decisions) {
    const patterns = {
      repeated_detections: {},
      zone_clustering: {},
      time_clustering: {},
      escalation_pattern: false,
    };

    // Count detections by type and zone
    decisions.forEach((decision) => {
      const key = `${decision.original_detection.object_type}_${decision.original_detection.zone}`;
      patterns.repeated_detections[key] =
        (patterns.repeated_detections[key] || 0) + 1;
    });

    // Check for escalation pattern
    const criticalCount = decisions.filter(
      (d) => d.business_priority === "critical",
    ).length;
    patterns.escalation_pattern = criticalCount > 1;

    return patterns;
  }

  calculateOverallThreat(decisions) {
    const priorityWeights = { low: 1, medium: 2, high: 3, critical: 4 };
    const totalWeight = decisions.reduce((sum, decision) => {
      return sum + priorityWeights[decision.business_priority];
    }, 0);

    const avgWeight = totalWeight / decisions.length;

    if (avgWeight >= 3.5) return "critical";
    if (avgWeight >= 2.5) return "high";
    if (avgWeight >= 1.5) return "medium";
    return "low";
  }

  recommendActions(decisions, patterns) {
    const actions = [];

    if (patterns.escalation_pattern) {
      actions.push({
        type: "escalate_to_security",
        priority: "immediate",
        reason: "Multiple critical events detected",
      });
    }

    const highPriorityCount = decisions.filter((d) =>
      ["high", "critical"].includes(d.business_priority),
    ).length;

    if (highPriorityCount > 2) {
      actions.push({
        type: "notify_management",
        priority: "urgent",
        reason: "Multiple high-priority security events",
      });
    }

    return actions;
  }
}

// Main execution
async function main() {
  const inputFile = process.argv
    .find((arg) => arg.startsWith("--input"))
    ?.split("=")[1];
  const outputFile = process.argv
    .find((arg) => arg.startsWith("--output"))
    ?.split("=")[1];

  if (!inputFile) {
    console.error("Input file required: --input=file.json");
    process.exit(1);
  }

  try {
    const inputData = JSON.parse(fs.readFileSync(inputFile, "utf8"));
    const engine = new BusinessRulesEngine();

    let result;
    if (Array.isArray(inputData.detections)) {
      result = engine.processAggregatedEvents(inputData.detections);
    } else if (inputData.enhanced_detections) {
      result = engine.processAggregatedEvents(inputData.enhanced_detections);
    } else {
      result = engine.processDetection(inputData);
    }

    const output = {
      timestamp: new Date().toISOString(),
      processor: "business_rules_engine",
      input_source: inputData.source || "unknown",
      business_analysis: result,
    };

    if (outputFile) {
      fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
    } else {
      console.log(JSON.stringify(output, null, 2));
    }
  } catch (error) {
    const errorOutput = {
      error: error.message,
      processor: "business_rules_engine",
      success: false,
    };
    console.error(JSON.stringify(errorOutput, null, 2));
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { BusinessRulesEngine };
```

## scripts/sensor_analytics.py

```python
#!/usr/bin/env python3
"""
Python Sensor Analytics Processor
Performs anomaly detection and statistical analysis on sensor data
"""

import sys
import json
import argparse
import os
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import deque
import statistics

class SensorAnalytics:
    def __init__(self):
        self.anomaly_threshold = float(os.getenv('CONFIG_ANOMALY_THRESHOLD', '2.5'))
        self.window_size = int(os.getenv('CONFIG_WINDOW_SIZE', '100'))
        self.min_samples = int(os.getenv('CONFIG_MIN_SAMPLES', '10'))

        # In-memory storage for this session
        self.sensor_history = {}

    def detect_anomalies(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect anomalies in sensor data using statistical methods"""
        anomalies = []
        processed_sensors = {}

        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            value = reading.get('value', 0)
            timestamp = reading.get('timestamp', datetime.now().isoformat())

            if sensor_id not in self.sensor_history:
                self.sensor_history[sensor_id] = deque(maxlen=self.window_size)

            history = self.sensor_history[sensor_id]
            history.append(value)

            if len(history) >= self.min_samples:
                # Calculate z-score for anomaly detection
                mean_val = statistics.mean(history)
                std_val = statistics.stdev(history) if len(history) > 1 else 0

                if std_val > 0:
                    z_score = abs(value - mean_val) / std_val
                    is_anomaly = z_score > self.anomaly_threshold
                else:
                    is_anomaly = False
                    z_score = 0

                sensor_stats = {
                    'sensor_id': sensor_id,
                    'current_value': value,
                    'mean': mean_val,
                    'std': std_val,
                    'z_score': z_score,
                    'is_anomaly': is_anomaly,
                    'timestamp': timestamp,
                    'sample_count': len(history)
                }

                processed_sensors[sensor_id] = sensor_stats

                if is_anomaly:
                    anomalies.append({
                        'sensor_id': sensor_id,
                        'value': value,
                        'expected_range': [mean_val - 2*std_val, mean_val + 2*std_val],
                        'z_score': z_score,
                        'severity': self.classify_severity(z_score),
                        'timestamp': timestamp
                    })

        return {
            'anomalies': anomalies,
            'sensor_statistics': processed_sensors,
            'total_sensors': len(processed_sensors),
            'anomaly_count': len(anomalies)
        }

    def classify_severity(self, z_score: float) -> str:
        """Classify anomaly severity based on z-score"""
        if z_score > 4:
            return 'critical'
        elif z_score > 3:
            return 'high'
        elif z_score > 2.5:
            return 'medium'
        else:
            return 'low'

    def analyze_trends(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze trends in sensor data"""
        trends = {}

        # Group by sensor
        sensors = {}
        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            if sensor_id not in sensors:
                sensors[sensor_id] = []
            sensors[sensor_id].append(reading)

        for sensor_id, readings in sensors.items():
            if len(readings) < 3:
                continue

            values = [r.get('value', 0) for r in readings]
            timestamps = [r.get('timestamp') for r in readings]

            # Simple trend analysis
            if len(values) >= 2:
                trend_direction = 'stable'
                if values[-1] > values[0]:
                    trend_direction = 'increasing'
                elif values[-1] < values[0]:
                    trend_direction = 'decreasing'

                rate_of_change = (values[-1] - values[0]) / len(values)

                trends[sensor_id] = {
                    'direction': trend_direction,
                    'rate_of_change': rate_of_change,
                    'min_value': min(values),
                    'max_value': max(values),
                    'avg_value': statistics.mean(values),
                    'value_range': max(values) - min(values),
                    'sample_count': len(values)
                }

        return trends

    def generate_alerts(self, anomalies: List[Dict[str, Any]], trends: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate alerts based on anomalies and trends"""
        alerts = []

        # Anomaly-based alerts
        for anomaly in anomalies:
            if anomaly['severity'] in ['critical', 'high']:
                alerts.append({
                    'type': 'anomaly_alert',
                    'sensor_id': anomaly['sensor_id'],
                    'message': f"Anomaly detected: {anomaly['sensor_id']} value {anomaly['value']} (z-score: {anomaly['z_score']:.2f})",
                    'severity': anomaly['severity'],
                    'timestamp': anomaly['timestamp'],
                    'recommended_action': 'investigate_sensor' if anomaly['severity'] == 'critical' else 'monitor_closely'
                })

        # Trend-based alerts
        for sensor_id, trend in trends.items():
            if abs(trend['rate_of_change']) > 10:  # Configurable threshold
                alerts.append({
                    'type': 'trend_alert',
                    'sensor_id': sensor_id,
                    'message': f"Rapid change detected: {sensor_id} changing at rate {trend['rate_of_change']:.2f}",
                    'severity': 'medium',
                    'timestamp': datetime.now().isoformat(),
                    'trend_direction': trend['direction'],
                    'recommended_action': 'check_sensor_calibration'
                })

        return alerts

def main():
    parser = argparse.ArgumentParser(description='Sensor Analytics Processor')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        analytics = SensorAnalytics()

        # Extract sensor data from input
        if 'events' in input_data:
            sensor_data = input_data['events']
        elif isinstance(input_data, list):
            sensor_data = input_data
        else:
            sensor_data = [input_data]

        # Perform analysis
        anomaly_results = analytics.detect_anomalies(sensor_data)
        trend_results = analytics.analyze_trends(sensor_data)
        alerts = analytics.generate_alerts(anomaly_results['anomalies'], trend_results)

        output = {
            'timestamp': datetime.now().isoformat(),
            'processor': 'sensor_analytics',
            'input_count': len(sensor_data),
            'anomaly_analysis': anomaly_results,
            'trend_analysis': trend_results,
            'alerts': alerts,
            'summary': {
                'total_anomalies': len(anomaly_results['anomalies']),
                'critical_anomalies': len([a for a in anomaly_results['anomalies'] if a['severity'] == 'critical']),
                'total_alerts': len(alerts),
                'sensors_analyzed': len(trend_results)
            }
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'sensor_analytics',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/grpc_ml_client.py

```python
#!/usr/bin/env python3
"""
gRPC ML Client for remote inference
Connects to ML serving infrastructure
"""

import sys
import json
import argparse
import os
import grpc
import base64
import numpy as np
from typing import Dict, Any, Optional

# Note: In real implementation, you would have generated protobuf files
# For this example, we'll use a simple REST fallback

class GRPCMLClient:
    def __init__(self):
        self.server_address = os.getenv('CONFIG_GRPC_SERVER', 'localhost:50051')
        self.model_name = os.getenv('CONFIG_MODEL_NAME', 'object_detection')
        self.timeout = int(os.getenv('CONFIG_TIMEOUT', '30'))
        self.use_fallback = True  # Use REST fallback for this example

    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Send prediction request to gRPC server"""
        if self.use_fallback:
            return self._rest_fallback(input_data)
        else:
            return self._grpc_predict(input_data)

    def _grpc_predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Real gRPC prediction (requires protobuf definitions)"""
        try:
            # This would be the real gRPC implementation
            # with grpc.insecure_channel(self.server_address) as channel:
            #     stub = PredictionServiceStub(channel)
            #     request = PredictRequest(...)
            #     response = stub.Predict(request, timeout=self.timeout)
            #     return self._parse_response(response)

            # Placeholder for actual gRPC implementation
            return {
                'error': 'gRPC implementation requires protobuf definitions',
                'fallback_used': True
            }

        except Exception as e:
            return {
                'error': f'gRPC prediction failed: {str(e)}',
                'success': False
            }

    def _rest_fallback(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """REST API fallback for ML inference"""
        import requests

        try:
            # Convert gRPC server address to HTTP
            if '://' not in self.server_address:
                url = f"http://{self.server_address}/predict"
            else:
                url = self.server_address.replace('grpc://', 'http://') + '/predict'

            # Prepare payload
            payload = {
                'model_name': self.model_name,
                'inputs': input_data,
                'parameters': {
                    'timeout': self.timeout
                }
            }

            response = requests.post(
                url,
                json=payload,
                timeout=self.timeout,
                headers={'Content-Type': 'application/json'}
            )

            if response.status_code == 200:
                return response.json()
            else:
                return {
                    'error': f'HTTP {response.status_code}: {response.text}',
                    'success': False
                }

        except requests.exceptions.RequestException as e:
            # If real server not available, return mock prediction
            return self._mock_prediction(input_data)
        except Exception as e:
            return {
                'error': f'REST fallback failed: {str(e)}',
                'success': False
            }

    def _mock_prediction(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Mock prediction for testing"""
        return {
            'model_name': self.model_name,
            'predictions': [
                {
                    'class': 'person',
                    'confidence': 0.92,
                    'bbox': [100, 50, 200, 300]
                },
                {
                    'class': 'car',
                    'confidence': 0.78,
                    'bbox': [300, 200, 500, 400]
                }
            ],
            'inference_time_ms': 45.2,
            'model_version': 'v1.0',
            'server_info': {
                'server': 'mock_server',
                'status': 'healthy'
            },
            'mock_used': True
        }

    def batch_predict(self, batch_data: list) -> Dict[str, Any]:
        """Process batch of predictions"""
        results = []
        total_time = 0

        for i, data in enumerate(batch_data):
            result = self.predict(data)
            results.append({
                'batch_index': i,
                'result': result
            })

            # Accumulate inference time
            if 'inference_time_ms' in result:
                total_time += result['inference_time_ms']

        return {
            'batch_results': results,
            'batch_size': len(batch_data),
            'total_inference_time_ms': total_time,
            'average_inference_time_ms': total_time / len(batch_data) if batch_data else 0
        }

def main():
    parser = argparse.ArgumentParser(description='gRPC ML Client')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')
    parser.add_argument('--batch', action='store_true', help='Process as batch')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        client = GRPCMLClient()

        if args.batch and isinstance(input_data, list):
            result = client.batch_predict(input_data)
        else:
            result = client.predict(input_data)

        output = {
            'timestamp': '2024-01-01T12:00:00Z',
            'processor': 'grpc_ml_client',
            'server': client.server_address,
            'model': client.model_name,
            'ml_inference': result
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'grpc_ml_client',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/alerting.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Alerting Engine
 * Handles multi-channel notifications and escalation
 */

const fs = require("fs");
const https = require("https");
const http = require("http");

const config = {
  alertChannels: (process.env.CONFIG_ALERT_CHANNELS || "email,webhook").split(
    ",",
  ),
  escalationRules:
    process.env.CONFIG_ESCALATION_RULES ||
    "immediate:critical,5min:high,1h:medium",
  webhookUrl: process.env.CONFIG_WEBHOOK_URL || "http://localhost:8080/webhook",
  slackWebhook: process.env.CONFIG_SLACK_WEBHOOK || "",
  maxRetries: parseInt(process.env.CONFIG_MAX_RETRIES || "3"),
};

class AlertingEngine {
  constructor() {
    this.escalationRules = this.parseEscalationRules();
    this.alertHistory = new Map();
  }

  parseEscalationRules() {
    const rules = {};
    config.escalationRules.split(",").forEach((rule) => {
      const [time, severity] = rule.split(":");
      rules[severity] = this.parseTimeToSeconds(time);
    });
    return rules;
  }

  parseTimeToSeconds(timeStr) {
    if (timeStr === "immediate") return 0;

    const value = parseInt(timeStr);
    if (timeStr.endsWith("min")) return value * 60;
    if (timeStr.endsWith("h")) return value * 3600;
    if (timeStr.endsWith("s")) return value;
    return value; // assume seconds
  }

  async processAlerts(alertData) {
    const alerts = Array.isArray(alertData.alerts)
      ? alertData.alerts
      : [alertData];
    const results = [];

    for (const alert of alerts) {
      const result = await this.processAlert(alert);
      results.push(result);
    }

    return {
      processed_alerts: results,
      total_alerts: alerts.length,
      successful_deliveries: results.filter((r) => r.success).length,
      failed_deliveries: results.filter((r) => !r.success).length,
    };
  }

  async processAlert(alert) {
    const alertId = this.generateAlertId(alert);
    const severity = alert.severity || "medium";
    const escalationDelay = this.escalationRules[severity] || 300;

    const alertResult = {
      alert_id: alertId,
      severity: severity,
      escalation_delay: escalationDelay,
      channels_used: [],
      success: false,
      delivery_results: [],
    };

    // Immediate processing for critical alerts
    if (severity === "critical" || escalationDelay === 0) {
      return await this.deliverAlert(alert, alertResult);
    }

    // Schedule escalation for non-critical alerts
    this.scheduleEscalation(alert, alertResult, escalationDelay);

    return alertResult;
  }

  async deliverAlert(alert, alertResult) {
    const deliveryPromises = [];

    for (const channel of config.alertChannels) {
      switch (channel.trim()) {
        case "webhook":
          deliveryPromises.push(this.sendWebhook(alert));
          break;
        case "slack":
          if (config.slackWebhook) {
            deliveryPromises.push(this.sendSlack(alert));
          }
          break;
        case "email":
          deliveryPromises.push(this.sendEmail(alert));
          break;
        default:
          console.warn(`Unknown alert channel: ${channel}`);
      }
    }

    try {
      const results = await Promise.allSettled(deliveryPromises);

      results.forEach((result, index) => {
        const channel = config.alertChannels[index];
        alertResult.delivery_results.push({
          channel: channel,
          success: result.status === "fulfilled",
          error: result.status === "rejected" ? result.reason : null,
        });

        if (result.status === "fulfilled") {
          alertResult.channels_used.push(channel);
        }
      });

      alertResult.success = alertResult.channels_used.length > 0;
    } catch (error) {
      alertResult.error = error.message;
    }

    return alertResult;
  }

  async sendWebhook(alert) {
    return new Promise((resolve, reject) => {
      const payload = JSON.stringify({
        alert_type: "camel_router_alert",
        timestamp: new Date().toISOString(),
        severity: alert.severity,
        message: alert.message || "Alert triggered",
        source: alert.source || "camel_router",
        details: alert,
      });

      const url = new URL(config.webhookUrl);
      const options = {
        hostname: url.hostname,
        port: url.port || (url.protocol === "https:" ? 443 : 80),
        path: url.pathname + url.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = url.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ channel: "webhook", status: "success" });
        } else {
          reject(new Error(`Webhook failed with status ${res.statusCode}`));
        }
      });

      req.on("error", (error) => {
        reject(new Error(`Webhook request failed: ${error.message}`));
      });

      req.write(payload);
      req.end();
    });
  }

  async sendSlack(alert) {
    if (!config.slackWebhook) {
      throw new Error("Slack webhook URL not configured");
    }

    const payload = JSON.stringify({
      text: `🚨 Alert: ${alert.message || "Camel Router Alert"}`,
      attachments: [
        {
          color: this.getSeverityColor(alert.severity),
          fields: [
            { title: "Severity", value: alert.severity, short: true },
            {
              title: "Source",
              value: alert.source || "camel_router",
              short: true,
            },
            {
              title: "Timestamp",
              value: new Date().toISOString(),
              short: false,
            },
          ],
        },
      ],
    });

    return this.sendWebhookPayload(config.slackWebhook, payload);
  }

  async sendEmail(alert) {
    // Email sending would require additional setup (SMTP, etc.)
    // For now, we'll log and return success
    console.log(`📧 Email alert would be sent: ${alert.message}`);
    return { channel: "email", status: "simulated" };
  }

  getSeverityColor(severity) {
    const colors = {
      critical: "danger",
      high: "warning",
      medium: "warning",
      low: "good",
    };
    return colors[severity] || "warning";
  }

  generateAlertId(alert) {
    const hash = require("crypto")
      .createHash("md5")
      .update(JSON.stringify(alert) + Date.now())
      .digest("hex");
    return hash.substring(0, 8);
  }

  scheduleEscalation(alert, alertResult, delay) {
    setTimeout(async () => {
      console.log(`⏰ Escalating alert after ${delay}s delay`);
      await this.deliverAlert(alert, alertResult);
    }, delay * 1000);
  }

  async sendWebhookPayload(url, payload) {
    return new Promise((resolve, reject) => {
      const urlObj = new URL(url);
      const options = {
        hostname: urlObj.hostname,
        port: urlObj.port || (urlObj.protocol === "https:" ? 443 : 80),
        path: urlObj.pathname + urlObj.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = urlObj.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ status: "success" });
        } else {
          reject(new Error(`Request failed with status ${res.statusCode}`));
        }
      });

      req.on("error", reject);
      req.write(payload);
      req.end();
    });
  }
}

// Main execution
async function main() {
  const inputFile = process.argv
    .find((arg) => arg.startsWith("--input"))
    ?.split("=")[1];
  const outputFile = process.argv
    .find((arg) => arg.startsWith("--output"))
    ?.split("=")[1];

  if (!inputFile) {
    console.error("Input file required: --input=file.json");
    process.exit(1);
  }

  try {
    const inputData = JSON.parse(fs.readFileSync(inputFile, "utf8"));
    const engine = new AlertingEngine();

    const result = await engine.processAlerts(inputData);

    const output = {
      timestamp: new Date().toISOString(),
      processor: "alerting_engine",
      configuration: {
        channels: config.alertChannels,
        escalation_rules: config.escalationRules,
      },
      alerting_results: result,
    };

    if (outputFile) {
      fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
    } else {
      console.log(JSON.stringify(output, null, 2));
    }
  } catch (error) {
    const errorOutput = {
      error: error.message,
      processor: "alerting_engine",
      success: false,
    };
    console.error(JSON.stringify(errorOutput, null, 2));
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { AlertingEngine };
```

## scripts/cpp_processor.cpp

````cpp
/**
 * C++ High-Performance Post-Processor
 * Optimized algorithms for real-time processing
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <cmath>
#include <algorithm>
#include <chrono>
#include <map>
#include <sstream>

// Simple JSON-like structure for this example
struct Detection {
    std::string object_type;
    double confidence;
    std::vector<double> bbox;
    std::string position;
};

struct ProcessingResult {
    std::vector<Detection> optimized_detections;
    double processing_time_ms;
    int original_count;
    int filtered_count;
    std::string algorithm_used;
};

class CPPPostProcessor {
private:
    double nms_threshold;
    double confidence_threshold;
    std::string algorithm;

public:
    CPPPostProcessor() {
        // Load configuration from environment variables
        nms_threshold = std::getenv("CONFIG_NMS_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_NMS_THRESHOLD")) : 0.5;
        confidence_threshold = std::getenv("CONFIG_CONFIDENCE_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_CONFIDENCE_THRESHOLD")) : 0.6;
        algorithm = std::getenv("CONFIG_ALGORITHM") ?
            std::getenv("CONFIG_ALGORITHM") : "fast_nms";
    }

    ProcessingResult processDetections(const std::vector<Detection>& detections) {
        auto start = std::chrono::high_resolution_clock::now();

        std::vector<Detection> filtered = filterByConfidence(detections);
        std::vector<Detection> optimized;

        if (algorithm == "fast_nms") {
            optimized = fastNonMaxSuppression(filtered);
        } else if (algorithm == "sort_confidence") {
            optimized = sortByConfidence(filtered);
        } else {
            optimized = filtered;
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

        ProcessingResult result;
        result.optimized_detections = optimized;
        result.processing_time_ms = duration.count() / 1000.0;
        result.original_count = detections.size();
        result.filtered_count = optimized.size();
        result.algorithm_used = algorithm;

        return result;
    }

private:
    std::vector<Detection> filterByConfidence(const std::vector<Detection>& detections) {
        std::vector<Detection> filtered;
        for (const auto& det : detections) {
            if (det.confidence >= confidence_threshold) {
                filtered.push_back(det);
            }
        }
        return filtered;
    }

    # Missing External Scripts

## scripts/business_rules.js
```javascript
#!/usr/bin/env node
/**
 * Node.js Business Rules Processor
 * Handles complex business logic and decision making
 */

const fs = require('fs');
const path = require('path');

// Configuration from environment variables
const config = {
    rulesFile: process.env.CONFIG_RULES_FILE || 'security_rules.json',
    alertThreshold: parseFloat(process.env.CONFIG_ALERT_THRESHOLD || '0.7'),
    businessHours: process.env.CONFIG_BUSINESS_HOURS || '09:00-17:00',
    escalationTime: parseInt(process.env.CONFIG_ESCALATION_TIME || '300') // seconds
};

class BusinessRulesEngine {
    constructor() {
        this.rules = this.loadRules();
    }

    loadRules() {
        try {
            if (fs.existsSync(config.rulesFile)) {
                return JSON.parse(fs.readFileSync(config.rulesFile, 'utf8'));
            }
        } catch (error) {
            console.error(`Error loading rules: ${error.message}`);
        }

        // Default rules
        return {
            person_detection: {
                business_hours: { priority: 'medium', action: 'log' },
                after_hours: { priority: 'high', action: 'alert' },
                restricted_zones: { priority: 'critical', action: 'immediate_alert' }
            },
            vehicle_detection: {
                parking_area: { priority: 'low', action: 'log' },
                restricted_area: { priority: 'high', action: 'alert' }
            },
            threat_assessment: {
                high_confidence: { threshold: 0.8, action: 'escalate' },
                medium_confidence: { threshold: 0.6, action: 'monitor' },
                low_confidence: { threshold: 0.3, action: 'log' }
            }
        };
    }

    isBusinessHours() {
        const now = new Date();
        const currentTime = now.getHours() * 100 + now.getMinutes();
        const [start, end] = config.businessHours.split('-').map(time => {
            const [hours, minutes] = time.split(':').map(Number);
            return hours * 100 + minutes;
        });
        return currentTime >= start && currentTime <= end;
    }

    processDetection(detection) {
        const { object_type, confidence, zone, position } = detection;
        const isAfterHours = !this.isBusinessHours();

        let decision = {
            original_detection: detection,
            business_priority: 'low',
            recommended_action: 'log',
            escalation_required: false,
            business_context: {},
            timestamp: new Date().toISOString()
        };

        // Apply object-specific rules
        if (object_type === 'person') {
            if (zone === 'restricted' || zone === 'entrance') {
                decision.business_priority = 'critical';
                decision.recommended_action = 'immediate_alert';
                decision.escalation_required = true;
            } else if (isAfterHours) {
                decision.business_priority = 'high';
                decision.recommended_action = 'alert';
            } else {
                decision.business_priority = 'medium';
                decision.recommended_action = 'monitor';
            }
        }

        if (object_type === 'car') {
            if (zone === 'parking') {
                decision.business_priority = 'low';
                decision.recommended_action = 'log';
            } else {
                decision.business_priority = 'medium';
                decision.recommended_action = 'monitor';
            }
        }

        // Apply confidence-based rules
        if (confidence > 0.8) {
            decision.business_priority = this.raisePriority(decision.business_priority);
        }

        // Add business context
        decision.business_context = {
            is_business_hours: !isAfterHours,
            zone_risk_level: this.getZoneRiskLevel(zone),
            time_based_modifier: isAfterHours ? 1.5 : 1.0,
            requires_immediate_response: decision.business_priority === 'critical'
        };

        return decision;
    }

    raisePriority(currentPriority) {
        const priorities = ['low', 'medium', 'high', 'critical'];
        const currentIndex = priorities.indexOf(currentPriority);
        return priorities[Math.min(currentIndex + 1, priorities.length - 1)];
    }

    getZoneRiskLevel(zone) {
        const riskLevels = {
            'entrance': 'high',
            'restricted': 'critical',
            'parking': 'low',
            'perimeter': 'medium',
            'garden': 'low'
        };
        return riskLevels[zone] || 'medium';
    }

    processAggregatedEvents(events) {
        const decisions = events.map(event => this.processDetection(event));

        // Analyze patterns
        const patterns = this.analyzePatterns(decisions);

        return {
            individual_decisions: decisions,
            pattern_analysis: patterns,
            overall_threat_level: this.calculateOverallThreat(decisions),
            recommended_actions: this.recommendActions(decisions, patterns)
        };
    }

    analyzePatterns(decisions) {
        const patterns = {
            repeated_detections: {},
            zone_clustering: {},
            time_clustering: {},
            escalation_pattern: false
        };

        // Count detections by type and zone
        decisions.forEach(decision => {
            const key = `${decision.original_detection.object_type}_${decision.original_detection.zone}`;
            patterns.repeated_detections[key] = (patterns.repeated_detections[key] || 0) + 1;
        });

        // Check for escalation pattern
        const criticalCount = decisions.filter(d => d.business_priority === 'critical').length;
        patterns.escalation_pattern = criticalCount > 1;

        return patterns;
    }

    calculateOverallThreat(decisions) {
        const priorityWeights = { low: 1, medium: 2, high: 3, critical: 4 };
        const totalWeight = decisions.reduce((sum, decision) => {
            return sum + priorityWeights[decision.business_priority];
        }, 0);

        const avgWeight = totalWeight / decisions.length;

        if (avgWeight >= 3.5) return 'critical';
        if (avgWeight >= 2.5) return 'high';
        if (avgWeight >= 1.5) return 'medium';
        return 'low';
    }

    recommendActions(decisions, patterns) {
        const actions = [];

        if (patterns.escalation_pattern) {
            actions.push({
                type: 'escalate_to_security',
                priority: 'immediate',
                reason: 'Multiple critical events detected'
            });
        }

        const highPriorityCount = decisions.filter(d =>
            ['high', 'critical'].includes(d.business_priority)
        ).length;

        if (highPriorityCount > 2) {
            actions.push({
                type: 'notify_management',
                priority: 'urgent',
                reason: 'Multiple high-priority security events'
            });
        }

        return actions;
    }
}

// Main execution
async function main() {
    const inputFile = process.argv.find(arg => arg.startsWith('--input'))?.split('=')[1];
    const outputFile = process.argv.find(arg => arg.startsWith('--output'))?.split('=')[1];

    if (!inputFile) {
        console.error('Input file required: --input=file.json');
        process.exit(1);
    }

    try {
        const inputData = JSON.parse(fs.readFileSync(inputFile, 'utf8'));
        const engine = new BusinessRulesEngine();

        let result;
        if (Array.isArray(inputData.detections)) {
            result = engine.processAggregatedEvents(inputData.detections);
        } else if (inputData.enhanced_detections) {
            result = engine.processAggregatedEvents(inputData.enhanced_detections);
        } else {
            result = engine.processDetection(inputData);
        }

        const output = {
            timestamp: new Date().toISOString(),
            processor: 'business_rules_engine',
            input_source: inputData.source || 'unknown',
            business_analysis: result
        };

        if (outputFile) {
            fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
        } else {
            console.log(JSON.stringify(output, null, 2));
        }

    } catch (error) {
        const errorOutput = {
            error: error.message,
            processor: 'business_rules_engine',
            success: false
        };
        console.error(JSON.stringify(errorOutput, null, 2));
        process.exit(1);
    }
}

if (require.main === module) {
    main();
}

module.exports = { BusinessRulesEngine };
````

## scripts/sensor_analytics.py

```python
#!/usr/bin/env python3
"""
Python Sensor Analytics Processor
Performs anomaly detection and statistical analysis on sensor data
"""

import sys
import json
import argparse
import os
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import deque
import statistics

class SensorAnalytics:
    def __init__(self):
        self.anomaly_threshold = float(os.getenv('CONFIG_ANOMALY_THRESHOLD', '2.5'))
        self.window_size = int(os.getenv('CONFIG_WINDOW_SIZE', '100'))
        self.min_samples = int(os.getenv('CONFIG_MIN_SAMPLES', '10'))

        # In-memory storage for this session
        self.sensor_history = {}

    def detect_anomalies(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect anomalies in sensor data using statistical methods"""
        anomalies = []
        processed_sensors = {}

        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            value = reading.get('value', 0)
            timestamp = reading.get('timestamp', datetime.now().isoformat())

            if sensor_id not in self.sensor_history:
                self.sensor_history[sensor_id] = deque(maxlen=self.window_size)

            history = self.sensor_history[sensor_id]
            history.append(value)

            if len(history) >= self.min_samples:
                # Calculate z-score for anomaly detection
                mean_val = statistics.mean(history)
                std_val = statistics.stdev(history) if len(history) > 1 else 0

                if std_val > 0:
                    z_score = abs(value - mean_val) / std_val
                    is_anomaly = z_score > self.anomaly_threshold
                else:
                    is_anomaly = False
                    z_score = 0

                sensor_stats = {
                    'sensor_id': sensor_id,
                    'current_value': value,
                    'mean': mean_val,
                    'std': std_val,
                    'z_score': z_score,
                    'is_anomaly': is_anomaly,
                    'timestamp': timestamp,
                    'sample_count': len(history)
                }

                processed_sensors[sensor_id] = sensor_stats

                if is_anomaly:
                    anomalies.append({
                        'sensor_id': sensor_id,
                        'value': value,
                        'expected_range': [mean_val - 2*std_val, mean_val + 2*std_val],
                        'z_score': z_score,
                        'severity': self.classify_severity(z_score),
                        'timestamp': timestamp
                    })

        return {
            'anomalies': anomalies,
            'sensor_statistics': processed_sensors,
            'total_sensors': len(processed_sensors),
            'anomaly_count': len(anomalies)
        }

    def classify_severity(self, z_score: float) -> str:
        """Classify anomaly severity based on z-score"""
        if z_score > 4:
            return 'critical'
        elif z_score > 3:
            return 'high'
        elif z_score > 2.5:
            return 'medium'
        else:
            return 'low'

    def analyze_trends(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze trends in sensor data"""
        trends = {}

        # Group by sensor
        sensors = {}
        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            if sensor_id not in sensors:
                sensors[sensor_id] = []
            sensors[sensor_id].append(reading)

        for sensor_id, readings in sensors.items():
            if len(readings) < 3:
                continue

            values = [r.get('value', 0) for r in readings]
            timestamps = [r.get('timestamp') for r in readings]

            # Simple trend analysis
            if len(values) >= 2:
                trend_direction = 'stable'
                if values[-1] > values[0]:
                    trend_direction = 'increasing'
                elif values[-1] < values[0]:
                    trend_direction = 'decreasing'

                rate_of_change = (values[-1] - values[0]) / len(values)

                trends[sensor_id] = {
                    'direction': trend_direction,
                    'rate_of_change': rate_of_change,
                    'min_value': min(values),
                    'max_value': max(values),
                    'avg_value': statistics.mean(values),
                    'value_range': max(values) - min(values),
                    'sample_count': len(values)
                }

        return trends

    def generate_alerts(self, anomalies: List[Dict[str, Any]], trends: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate alerts based on anomalies and trends"""
        alerts = []

        # Anomaly-based alerts
        for anomaly in anomalies:
            if anomaly['severity'] in ['critical', 'high']:
                alerts.append({
                    'type': 'anomaly_alert',
                    'sensor_id': anomaly['sensor_id'],
                    'message': f"Anomaly detected: {anomaly['sensor_id']} value {anomaly['value']} (z-score: {anomaly['z_score']:.2f})",
                    'severity': anomaly['severity'],
                    'timestamp': anomaly['timestamp'],
                    'recommended_action': 'investigate_sensor' if anomaly['severity'] == 'critical' else 'monitor_closely'
                })

        # Trend-based alerts
        for sensor_id, trend in trends.items():
            if abs(trend['rate_of_change']) > 10:  # Configurable threshold
                alerts.append({
                    'type': 'trend_alert',
                    'sensor_id': sensor_id,
                    'message': f"Rapid change detected: {sensor_id} changing at rate {trend['rate_of_change']:.2f}",
                    'severity': 'medium',
                    'timestamp': datetime.now().isoformat(),
                    'trend_direction': trend['direction'],
                    'recommended_action': 'check_sensor_calibration'
                })

        return alerts

def main():
    parser = argparse.ArgumentParser(description='Sensor Analytics Processor')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        analytics = SensorAnalytics()

        # Extract sensor data from input
        if 'events' in input_data:
            sensor_data = input_data['events']
        elif isinstance(input_data, list):
            sensor_data = input_data
        else:
            sensor_data = [input_data]

        # Perform analysis
        anomaly_results = analytics.detect_anomalies(sensor_data)
        trend_results = analytics.analyze_trends(sensor_data)
        alerts = analytics.generate_alerts(anomaly_results['anomalies'], trend_results)

        output = {
            'timestamp': datetime.now().isoformat(),
            'processor': 'sensor_analytics',
            'input_count': len(sensor_data),
            'anomaly_analysis': anomaly_results,
            'trend_analysis': trend_results,
            'alerts': alerts,
            'summary': {
                'total_anomalies': len(anomaly_results['anomalies']),
                'critical_anomalies': len([a for a in anomaly_results['anomalies'] if a['severity'] == 'critical']),
                'total_alerts': len(alerts),
                'sensors_analyzed': len(trend_results)
            }
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'sensor_analytics',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/grpc_ml_client.py

```python
#!/usr/bin/env python3
"""
gRPC ML Client for remote inference
Connects to ML serving infrastructure
"""

import sys
import json
import argparse
import os
import grpc
import base64
import numpy as np
from typing import Dict, Any, Optional

# Note: In real implementation, you would have generated protobuf files
# For this example, we'll use a simple REST fallback

class GRPCMLClient:
    def __init__(self):
        self.server_address = os.getenv('CONFIG_GRPC_SERVER', 'localhost:50051')
        self.model_name = os.getenv('CONFIG_MODEL_NAME', 'object_detection')
        self.timeout = int(os.getenv('CONFIG_TIMEOUT', '30'))
        self.use_fallback = True  # Use REST fallback for this example

    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Send prediction request to gRPC server"""
        if self.use_fallback:
            return self._rest_fallback(input_data)
        else:
            return self._grpc_predict(input_data)

    def _grpc_predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Real gRPC prediction (requires protobuf definitions)"""
        try:
            # This would be the real gRPC implementation
            # with grpc.insecure_channel(self.server_address) as channel:
            #     stub = PredictionServiceStub(channel)
            #     request = PredictRequest(...)
            #     response = stub.Predict(request, timeout=self.timeout)
            #     return self._parse_response(response)

            # Placeholder for actual gRPC implementation
            return {
                'error': 'gRPC implementation requires protobuf definitions',
                'fallback_used': True
            }

        except Exception as e:
            return {
                'error': f'gRPC prediction failed: {str(e)}',
                'success': False
            }

    def _rest_fallback(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """REST API fallback for ML inference"""
        import requests

        try:
            # Convert gRPC server address to HTTP
            if '://' not in self.server_address:
                url = f"http://{self.server_address}/predict"
            else:
                url = self.server_address.replace('grpc://', 'http://') + '/predict'

            # Prepare payload
            payload = {
                'model_name': self.model_name,
                'inputs': input_data,
                'parameters': {
                    'timeout': self.timeout
                }
            }

            response = requests.post(
                url,
                json=payload,
                timeout=self.timeout,
                headers={'Content-Type': 'application/json'}
            )

            if response.status_code == 200:
                return response.json()
            else:
                return {
                    'error': f'HTTP {response.status_code}: {response.text}',
                    'success': False
                }

        except requests.exceptions.RequestException as e:
            # If real server not available, return mock prediction
            return self._mock_prediction(input_data)
        except Exception as e:
            return {
                'error': f'REST fallback failed: {str(e)}',
                'success': False
            }

    def _mock_prediction(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Mock prediction for testing"""
        return {
            'model_name': self.model_name,
            'predictions': [
                {
                    'class': 'person',
                    'confidence': 0.92,
                    'bbox': [100, 50, 200, 300]
                },
                {
                    'class': 'car',
                    'confidence': 0.78,
                    'bbox': [300, 200, 500, 400]
                }
            ],
            'inference_time_ms': 45.2,
            'model_version': 'v1.0',
            'server_info': {
                'server': 'mock_server',
                'status': 'healthy'
            },
            'mock_used': True
        }

    def batch_predict(self, batch_data: list) -> Dict[str, Any]:
        """Process batch of predictions"""
        results = []
        total_time = 0

        for i, data in enumerate(batch_data):
            result = self.predict(data)
            results.append({
                'batch_index': i,
                'result': result
            })

            # Accumulate inference time
            if 'inference_time_ms' in result:
                total_time += result['inference_time_ms']

        return {
            'batch_results': results,
            'batch_size': len(batch_data),
            'total_inference_time_ms': total_time,
            'average_inference_time_ms': total_time / len(batch_data) if batch_data else 0
        }

def main():
    parser = argparse.ArgumentParser(description='gRPC ML Client')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')
    parser.add_argument('--batch', action='store_true', help='Process as batch')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        client = GRPCMLClient()

        if args.batch and isinstance(input_data, list):
            result = client.batch_predict(input_data)
        else:
            result = client.predict(input_data)

        output = {
            'timestamp': '2024-01-01T12:00:00Z',
            'processor': 'grpc_ml_client',
            'server': client.server_address,
            'model': client.model_name,
            'ml_inference': result
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'grpc_ml_client',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/alerting.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Alerting Engine
 * Handles multi-channel notifications and escalation
 */

const fs = require("fs");
const https = require("https");
const http = require("http");

const config = {
  alertChannels: (process.env.CONFIG_ALERT_CHANNELS || "email,webhook").split(
    ",",
  ),
  escalationRules:
    process.env.CONFIG_ESCALATION_RULES ||
    "immediate:critical,5min:high,1h:medium",
  webhookUrl: process.env.CONFIG_WEBHOOK_URL || "http://localhost:8080/webhook",
  slackWebhook: process.env.CONFIG_SLACK_WEBHOOK || "",
  maxRetries: parseInt(process.env.CONFIG_MAX_RETRIES || "3"),
};

class AlertingEngine {
  constructor() {
    this.escalationRules = this.parseEscalationRules();
    this.alertHistory = new Map();
  }

  parseEscalationRules() {
    const rules = {};
    config.escalationRules.split(",").forEach((rule) => {
      const [time, severity] = rule.split(":");
      rules[severity] = this.parseTimeToSeconds(time);
    });
    return rules;
  }

  parseTimeToSeconds(timeStr) {
    if (timeStr === "immediate") return 0;

    const value = parseInt(timeStr);
    if (timeStr.endsWith("min")) return value * 60;
    if (timeStr.endsWith("h")) return value * 3600;
    if (timeStr.endsWith("s")) return value;
    return value; // assume seconds
  }

  async processAlerts(alertData) {
    const alerts = Array.isArray(alertData.alerts)
      ? alertData.alerts
      : [alertData];
    const results = [];

    for (const alert of alerts) {
      const result = await this.processAlert(alert);
      results.push(result);
    }

    return {
      processed_alerts: results,
      total_alerts: alerts.length,
      successful_deliveries: results.filter((r) => r.success).length,
      failed_deliveries: results.filter((r) => !r.success).length,
    };
  }

  async processAlert(alert) {
    const alertId = this.generateAlertId(alert);
    const severity = alert.severity || "medium";
    const escalationDelay = this.escalationRules[severity] || 300;

    const alertResult = {
      alert_id: alertId,
      severity: severity,
      escalation_delay: escalationDelay,
      channels_used: [],
      success: false,
      delivery_results: [],
    };

    // Immediate processing for critical alerts
    if (severity === "critical" || escalationDelay === 0) {
      return await this.deliverAlert(alert, alertResult);
    }

    // Schedule escalation for non-critical alerts
    this.scheduleEscalation(alert, alertResult, escalationDelay);

    return alertResult;
  }

  async deliverAlert(alert, alertResult) {
    const deliveryPromises = [];

    for (const channel of config.alertChannels) {
      switch (channel.trim()) {
        case "webhook":
          deliveryPromises.push(this.sendWebhook(alert));
          break;
        case "slack":
          if (config.slackWebhook) {
            deliveryPromises.push(this.sendSlack(alert));
          }
          break;
        case "email":
          deliveryPromises.push(this.sendEmail(alert));
          break;
        default:
          console.warn(`Unknown alert channel: ${channel}`);
      }
    }

    try {
      const results = await Promise.allSettled(deliveryPromises);

      results.forEach((result, index) => {
        const channel = config.alertChannels[index];
        alertResult.delivery_results.push({
          channel: channel,
          success: result.status === "fulfilled",
          error: result.status === "rejected" ? result.reason : null,
        });

        if (result.status === "fulfilled") {
          alertResult.channels_used.push(channel);
        }
      });

      alertResult.success = alertResult.channels_used.length > 0;
    } catch (error) {
      alertResult.error = error.message;
    }

    return alertResult;
  }

  async sendWebhook(alert) {
    return new Promise((resolve, reject) => {
      const payload = JSON.stringify({
        alert_type: "camel_router_alert",
        timestamp: new Date().toISOString(),
        severity: alert.severity,
        message: alert.message || "Alert triggered",
        source: alert.source || "camel_router",
        details: alert,
      });

      const url = new URL(config.webhookUrl);
      const options = {
        hostname: url.hostname,
        port: url.port || (url.protocol === "https:" ? 443 : 80),
        path: url.pathname + url.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = url.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ channel: "webhook", status: "success" });
        } else {
          reject(new Error(`Webhook failed with status ${res.statusCode}`));
        }
      });

      req.on("error", (error) => {
        reject(new Error(`Webhook request failed: ${error.message}`));
      });

      req.write(payload);
      req.end();
    });
  }

  async sendSlack(alert) {
    if (!config.slackWebhook) {
      throw new Error("Slack webhook URL not configured");
    }

    const payload = JSON.stringify({
      text: `🚨 Alert: ${alert.message || "Camel Router Alert"}`,
      attachments: [
        {
          color: this.getSeverityColor(alert.severity),
          fields: [
            { title: "Severity", value: alert.severity, short: true },
            {
              title: "Source",
              value: alert.source || "camel_router",
              short: true,
            },
            {
              title: "Timestamp",
              value: new Date().toISOString(),
              short: false,
            },
          ],
        },
      ],
    });

    return this.sendWebhookPayload(config.slackWebhook, payload);
  }

  async sendEmail(alert) {
    // Email sending would require additional setup (SMTP, etc.)
    // For now, we'll log and return success
    console.log(`📧 Email alert would be sent: ${alert.message}`);
    return { channel: "email", status: "simulated" };
  }

  getSeverityColor(severity) {
    const colors = {
      critical: "danger",
      high: "warning",
      medium: "warning",
      low: "good",
    };
    return colors[severity] || "warning";
  }

  generateAlertId(alert) {
    const hash = require("crypto")
      .createHash("md5")
      .update(JSON.stringify(alert) + Date.now())
      .digest("hex");
    return hash.substring(0, 8);
  }

  scheduleEscalation(alert, alertResult, delay) {
    setTimeout(async () => {
      console.log(`⏰ Escalating alert after ${delay}s delay`);
      await this.deliverAlert(alert, alertResult);
    }, delay * 1000);
  }

  async sendWebhookPayload(url, payload) {
    return new Promise((resolve, reject) => {
      const urlObj = new URL(url);
      const options = {
        hostname: urlObj.hostname,
        port: urlObj.port || (urlObj.protocol === "https:" ? 443 : 80),
        path: urlObj.pathname + urlObj.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = urlObj.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ status: "success" });
        } else {
          reject(new Error(`Request failed with status ${res.statusCode}`));
        }
      });

      req.on("error", reject);
      req.write(payload);
      req.end();
    });
  }
}

// Main execution
async function main() {
  const inputFile = process.argv
    .find((arg) => arg.startsWith("--input"))
    ?.split("=")[1];
  const outputFile = process.argv
    .find((arg) => arg.startsWith("--output"))
    ?.split("=")[1];

  if (!inputFile) {
    console.error("Input file required: --input=file.json");
    process.exit(1);
  }

  try {
    const inputData = JSON.parse(fs.readFileSync(inputFile, "utf8"));
    const engine = new AlertingEngine();

    const result = await engine.processAlerts(inputData);

    const output = {
      timestamp: new Date().toISOString(),
      processor: "alerting_engine",
      configuration: {
        channels: config.alertChannels,
        escalation_rules: config.escalationRules,
      },
      alerting_results: result,
    };

    if (outputFile) {
      fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
    } else {
      console.log(JSON.stringify(output, null, 2));
    }
  } catch (error) {
    const errorOutput = {
      error: error.message,
      processor: "alerting_engine",
      success: false,
    };
    console.error(JSON.stringify(errorOutput, null, 2));
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { AlertingEngine };
```

## scripts/cpp_processor.cpp

```cpp
/**
 * C++ High-Performance Post-Processor
 * Optimized algorithms for real-time processing
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <cmath>
#include <algorithm>
#include <chrono>
#include <map>
#include <sstream>

// Simple JSON-like structure for this example
struct Detection {
    std::string object_type;
    double confidence;
    std::vector<double> bbox;
    std::string position;
};

struct ProcessingResult {
    std::vector<Detection> optimized_detections;
    double processing_time_ms;
    int original_count;
    int filtered_count;
    std::string algorithm_used;
};

class CPPPostProcessor {
private:
    double nms_threshold;
    double confidence_threshold;
    std::string algorithm;

public:
    CPPPostProcessor() {
        // Load configuration from environment variables
        nms_threshold = std::getenv("CONFIG_NMS_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_NMS_THRESHOLD")) : 0.5;
        confidence_threshold = std::getenv("CONFIG_CONFIDENCE_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_CONFIDENCE_THRESHOLD")) : 0.6;
        algorithm = std::getenv("CONFIG_ALGORITHM") ?
            std::getenv("CONFIG_ALGORITHM") : "fast_nms";
    }

    ProcessingResult processDetections(const std::vector<Detection>& detections) {
        auto start = std::chrono::high_resolution_clock::now();

        std::vector<Detection> filtered = filterByConfidence(detections);
        std::vector<Detection> optimized;

        if (algorithm == "fast_nms") {
            optimized = fastNonMaxSuppression(filtered);
        } else if (algorithm == "sort_confidence") {
            optimized = sortByConfidence(filtered);
        } else {
            optimized = filtered;
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

        ProcessingResult result;
        result.optimized_detections = optimized;
        result.processing_time_ms = duration.count() / 1000.0;
        result.original_count = detections.size();
        result.filtered_count = optimized.size();
        result.algorithm_used = algorithm;

        return result;
    }

private:
    std::vector<Detection> filterByConfidence(const std::vector<Detection>& detections) {
        std::vector<Detection> filtered;
        for (const auto& det : detections) {
            if (det.confidence >= confidence_threshold) {
                filtered.push_back(det);
            }
        }
        return filtered;
    }

    std::vector<Detection> fastNonMaxSuppression(const std::vector<Detection>& detections) {
        if (detections.empty()) return {};

        std::vector<Detection> sorted_detections = detections;
        std::sort(sorted_detections.begin(), sorted_detections.end(),
                  [](const Detection& a, const Detection& b) {
                      return a.confidence > b.confidence;
                  });

        std::vector<Detection> result;
        std::vector<bool> suppressed(sorted_detections.size(), false);

        for (size_t i = 0; i < sorted_detections.size(); ++i) {
            if (suppressed[i]) continue;

            result.push_back(sorted_detections[i]);

            for (size_t j = i + 1; j < sorted_detections.size(); ++j) {
                if (suppressed[j]) continue;

                double iou = calculateIoU(sorted_detections[i].bbox, sorted_detections[j].bbox);
                if (iou > nms_threshold) {
                    suppressed[j] = true;
                }
            }
        }

        return result;
    }

    std::vector<Detection> sortByConfidence(const std::vector<Detection>& detections) {
        std::vector<Detection> sorted_detections = detections;
        std::sort(sorted_detections.begin(), sorted_detections.end(),
                  [](const Detection& a, const Detection& b) {
                      return a.confidence > b.confidence;
                  });
        return sorted_detections;
    }

    double calculateIoU(const std::vector<double>& box1, const std::vector<double>& box2) {
        if (box1.size() < 4 || box2.size() < 4) return 0.0;

        double x1 = std::max(box1[0], box2[0]);
        double y1 = std::max(box1[1], box2[1]);
        double x2 = std::min(box1[2], box2[2]);
        double y2 = std::min(box1[3], box2[3]);

        if (x2 <= x1 || y2 <= y1) return 0.0;

        double intersection = (x2 - x1) * (y2 - y1);
        double area1 = (box1[2] - box1[0]) * (box1[3] - box1[1]);
        double area2 = (box2[2] - box2[0]) * (box2[3] - box2[1]);
        double union_area = area1 + area2 - intersection;

        return union_area > 0 ? intersection / union_area : 0.0;
    }
};

// Simple JSON parsing (in real implementation, use proper JSON library)
std::vector<Detection> parseDetections(const std::string& json_str) {
    std::vector<Detection> detections;

    // This is a simplified parser - in production use nlohmann/json or similar
    // For demo purposes, create mock detections
    Detection det1;
    det1.object_type = "person";
    det1.confidence = 0.85;
    det1.bbox = {100, 100, 200, 300};
    det1.position = "center";

    Detection det2;
    det2.object_type = "car";
    det2.confidence = 0.92;
    det2.bbox = {300, 150, 450, 280};
    det2.position = "right";

    detections.push_back(det1);
    detections.push_back(det2);

    return detections;
}

std::string formatOutput(const ProcessingResult& result) {
    std::ostringstream oss;
    oss << "{\n";
    oss << "  \"timestamp\": \"2024-01-01T12:00:00Z\",\n";
    oss << "  \"processor\": \"cpp_postprocessor\",\n";
    oss << "  \"algorithm_used\": \"" << result.algorithm_used << "\",\n";
    oss << "  \"processing_time_ms\": " << result.processing_time_ms << ",\n";
    oss << "  \"original_count\": " << result.original_count << ",\n";
    oss << "  \"filtered_count\": " << result.filtered_count << ",\n";
    oss << "  \"optimized_detections\": [\n";

    for (size_t i = 0; i < result.optimized_detections.size(); ++i) {
        const auto& det = result.optimized_detections[i];
        oss << "    {\n";
        oss << "      \"object_type\": \"" << det.object_type << "\",\n";
        oss << "      \"confidence\": " << det.confidence << ",\n";
        oss << "      \"position\": \"" << det.position << "\",\n";
        oss << "      \"bbox\": [" << det.bbox[0] << ", " << det.bbox[1]
            << ", " << det.bbox[2] << ", " << det.bbox[3] << "]\n";
        oss << "    }";
        if (i < result.optimized_detections.size() - 1) oss << ",";
        oss << "\n";
    }

    oss << "  ]\n";
    oss << "}\n";

    return oss.str();
}

int main(int argc, char* argv[]) {
    std::string input_file;
    std::string output_file;

    // Parse command line arguments
    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg.find("--input=") == 0) {
            input_file = arg.substr(8);
        } else if (arg.find("--output=") == 0) {
            output_file = arg.substr(9);
        }
    }

    if (input_file.empty()) {
        std::cerr << "Error: Input file required (--input=file.json)" << std::endl;
        return 1;
    }

    try {
        // Read input file
        std::ifstream file(input_file);
        if (!file.is_open()) {
            std::cerr << "Error: Cannot open input file: " << input_file << std::endl;
            return 1;
        }

        std::string json_content((std::istreambuf_iterator<char>(file)),
                                 std::istreambuf_iterator<char>());
        file.close();

        // Parse detections
        std::vector<Detection> detections = parseDetections(json_content);

        // Process with C++ optimizer
        CPPPostProcessor processor;
        ProcessingResult result = processor.processDetections(detections);

        // Format output
        std::string output = formatOutput(result);

        // Write output
        if (!output_file.empty()) {
            std::ofstream outfile(output_file);
            if (outfile.is_open()) {
                outfile << output;
                outfile.close();
            } else {
                std::cerr << "Error: Cannot write to output file: " << output_file << std::endl;
                return 1;
            }
        } else {
            std::cout << output;
        }

    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }

    return 0;
}
```

## scripts/Cargo.toml (Rust Project)

```toml
[package]
name = "camel-processors"
version = "0.1.0"
edition = "2021"

[[bin]]
name = "data_preprocessor"
path = "src/data_preprocessor.rs"

[[bin]]
name = "performance_analyzer"
path = "src/performance_analyzer.rs"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
clap = { version = "4.0", features = ["derive"] }
anyhow = "1.0"
chrono = { version = "0.4", features = ["serde"] }
rayon = "1.7"
ndarray = "0.15"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"
```

## scripts/src/data_preprocessor.rs (Rust)

```rust
//! High-performance data preprocessing in Rust
//! Optimized for SIMD operations and parallel processing

use anyhow::{Context, Result};
use clap::Parser;
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use std::fs;
use std::time::Instant;

#[derive(Parser, Debug)]
#[command(name = "data_preprocessor")]
#[command(about = "High-performance data preprocessing")]
struct Args {
    #[arg(long)]
    input: String,

    #[arg(long)]
    output: Option<String>,
}

#[derive(Debug, Deserialize)]
struct InputData {
    #[serde(default)]
    data: Vec<f64>,
    #[serde(default)]
    batch_data: Vec<Vec<f64>>,
    #[serde(default)]
    config: ProcessingConfig,
}

#[derive(Debug, Deserialize)]
struct ProcessingConfig {
    #[serde(default = "default_batch_size")]
    batch_size: usize,
    #[serde(default = "default_normalize")]
    normalize: bool,
    #[serde(default = "default_parallel")]
    parallel: bool,
    #[serde(default = "default_simd")]
    simd_enabled: bool,
}

fn default_batch_size() -> usize { 32 }
fn default_normalize() -> bool { true }
fn default_parallel() -> bool { true }
fn default_simd() -> bool { true }

impl Default for ProcessingConfig {
    fn default() -> Self {
        Self {
            batch_size: default_batch_size(),
            normalize: default_normalize(),
            parallel: default_parallel(),
            simd_enabled: default_simd(),
        }
    }
}

#[derive(Debug, Serialize)]
struct ProcessingResult {
    timestamp: String,
    processor: String,
    processing_time_ms: f64,
    input_size: usize,
    output_size: usize,
    batches_processed: usize,
    preprocessed_data: Vec<Vec<f64>>,
    statistics: DataStatistics,
}

#[derive(Debug, Serialize)]
struct DataStatistics {
    mean: f64,
    std_dev: f64,
    min: f64,
    max: f64,
    total_elements: usize,
}

struct DataPreprocessor {
    config: ProcessingConfig,
}

impl DataPreprocessor {
    fn new(config: ProcessingConfig) -> Self {
        Self { config }
    }

    fn preprocess(&self, input: InputData) -> Result<ProcessingResult> {
        let start_time = Instant::now();

        // Determine data to process
        let data_to_process = if !input.batch_data.is_empty() {
            input.batch_data
        } else if !input.data.is_empty() {
            vec![input.data]
        } else {
            return Err(anyhow::anyhow!("No data provided"));
        };

        // Process in batches
        let batches: Vec<Vec<Vec<f64>>> = data_to_process
            .chunks(self.config.batch_size)
            .map(|chunk| chunk.to_vec())
            .collect();

        let processed_batches: Vec<Vec<f64>> = if self.config.parallel {
            batches
                .into_par_iter()
                .map(|batch| self.process_batch(batch))
                .collect()
        } else {
            batches
                .into_iter()
                .map(|batch| self.process_batch(batch))
                .collect()
        };

        // Flatten results
        let flattened: Vec<f64> = processed_batches
            .iter()
            .flatten()
            .copied()
            .collect();

        // Calculate statistics
        let stats = self.calculate_statistics(&flattened);

        // Reshape back to batches
        let final_batches: Vec<Vec<f64>> = processed_batches;

        let processing_time = start_time.elapsed().as_secs_f64() * 1000.0;

        Ok(ProcessingResult {
            timestamp: chrono::Utc::now().to_rfc3339(),
            processor: "rust_data_preprocessor".to_string(),
            processing_time_ms: processing_time,
            input_size: data_to_process.len(),
            output_size: final_batches.len(),
            batches_processed: final_batches.len(),
            preprocessed_data: final_batches,
            statistics: stats,
        })
    }

    fn process_batch(&self, batch: Vec<Vec<f64>>) -> Vec<f64> {
        let flattened: Vec<f64> = batch.into_iter().flatten().collect();

        if self.config.normalize {
            self.normalize_data(flattened)
        } else {
            flattened
        }
    }

    fn normalize_data(&self, data: Vec<f64>) -> Vec<f64> {
        if data.is_empty() {
            return data;
        }

        let mean = data.iter().sum::<f64>() / data.len() as f64;
        let variance = data
            .iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / data.len() as f64;
        let std_dev = variance.sqrt();

        if std_dev == 0.0 {
            return data;
        }

        if self.config.simd_enabled {
            // Use SIMD-like operations with rayon
            data.into_par_iter()
                .map(|x| (x - mean) / std_dev)
                .collect()
        } else {
            data.into_iter()
                .map(|x| (x - mean) / std_dev)
                .collect()
        }
    }

    fn calculate_statistics(&self, data: &[f64]) -> DataStatistics {
        if data.is_empty() {
            return DataStatistics {
                mean: 0.0,
                std_dev: 0.0,
                min: 0.0,
                max: 0.0,
                total_elements: 0,
            };
        }

        let mean = data.iter().sum::<f64>() / data.len() as f64;
        let variance = data
            .iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / data.len() as f64;
        let std_dev = variance.sqrt();
        let min = data.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max = data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

        DataStatistics {
            mean,
            std_dev,
            min,
            max,
            total_elements: data.len(),
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    // Read input file
    let input_content = fs::read_to_string(&args.input)
        .with_context(|| format!("Failed to read input file: {}", args.input))?;

    // Parse input JSON
    let input_data: InputData = serde_json::from_str(&input_content)
        .with_context(|| "Failed to parse input JSON")?;

    // Load configuration from environment if available
    let mut config = input_data.config;
    if let Ok(batch_size) = std::env::var("CONFIG_BATCH_SIZE") {
        config.batch_size = batch_size.parse().unwrap_or(config.batch_size);
    }
    if let Ok(normalize) = std::env::var("CONFIG_NORMALIZE") {
        config.normalize = normalize.parse().unwrap_or(config.normalize);
    }

    // Process data
    let preprocessor = DataPreprocessor::new(config);
    let result = preprocessor.preprocess(input_data)?;

    // Output results
    let output_json = serde_json::to_string_pretty(&result)
        .with_context(|| "Failed to serialize output")?;

    if let Some(output_file) = args.output {
        fs::write(&output_file, output_json)
            .with_context(|| format!("Failed to write output file: {}", output_file))?;
    } else {
        println!("{}", output_json);
    }

    Ok(())
}
```

## scripts/go.mod

```go
module camel-processors

go 1.21

require (
    github.com/golang/protobuf v1.5.3
    google.golang.org/grpc v1.57.0
    google.golang.org/protobuf v1.31.0
)

require (
    golang.org/x/net v0.12.0 // indirect
    golang.org/x/sys v0.10.0 // indirect
    golang.org/x/text v0.11.0 // indirect
    google.golang.org/genproto/googleapis/rpc v0.0.0-20230726155614-23370e0ffb3e // indirect
)
```

## scripts/package.json

```json
{
  "name": "camel-router-processors",
  "version": "1.0.0",
  "description": "Node.js processors for Camel Router",
  "main": "business_rules.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "lint": "eslint *.js",
    "format": "prettier --write *.js"
  },
  "keywords": ["camel-router", "processing", "business-rules", "alerting"],
  "author": "Camel Router Team",
  "license": "MIT",
  "dependencies": {
    "axios": "^1.5.0",
    "lodash": "^4.17.21",
    "moment": "^2.29.4"
  },
  "devDependencies": {
    "eslint": "^8.47.0",
    "prettier": "^3.0.1"
  },
  "engines": {
    "node": ">=16.0.0"
  }
}
```

# Missing External Scripts

## scripts/business_rules.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Business Rules Processor
 * Handles complex business logic and decision making
 */

const fs = require("fs");
const path = require("path");

// Configuration from environment variables
const config = {
  rulesFile: process.env.CONFIG_RULES_FILE || "security_rules.json",
  alertThreshold: parseFloat(process.env.CONFIG_ALERT_THRESHOLD || "0.7"),
  businessHours: process.env.CONFIG_BUSINESS_HOURS || "09:00-17:00",
  escalationTime: parseInt(process.env.CONFIG_ESCALATION_TIME || "300"), // seconds
};

class BusinessRulesEngine {
  constructor() {
    this.rules = this.loadRules();
  }

  loadRules() {
    try {
      if (fs.existsSync(config.rulesFile)) {
        return JSON.parse(fs.readFileSync(config.rulesFile, "utf8"));
      }
    } catch (error) {
      console.error(`Error loading rules: ${error.message}`);
    }

    // Default rules
    return {
      person_detection: {
        business_hours: { priority: "medium", action: "log" },
        after_hours: { priority: "high", action: "alert" },
        restricted_zones: { priority: "critical", action: "immediate_alert" },
      },
      vehicle_detection: {
        parking_area: { priority: "low", action: "log" },
        restricted_area: { priority: "high", action: "alert" },
      },
      threat_assessment: {
        high_confidence: { threshold: 0.8, action: "escalate" },
        medium_confidence: { threshold: 0.6, action: "monitor" },
        low_confidence: { threshold: 0.3, action: "log" },
      },
    };
  }

  isBusinessHours() {
    const now = new Date();
    const currentTime = now.getHours() * 100 + now.getMinutes();
    const [start, end] = config.businessHours.split("-").map((time) => {
      const [hours, minutes] = time.split(":").map(Number);
      return hours * 100 + minutes;
    });
    return currentTime >= start && currentTime <= end;
  }

  processDetection(detection) {
    const { object_type, confidence, zone, position } = detection;
    const isAfterHours = !this.isBusinessHours();

    let decision = {
      original_detection: detection,
      business_priority: "low",
      recommended_action: "log",
      escalation_required: false,
      business_context: {},
      timestamp: new Date().toISOString(),
    };

    // Apply object-specific rules
    if (object_type === "person") {
      if (zone === "restricted" || zone === "entrance") {
        decision.business_priority = "critical";
        decision.recommended_action = "immediate_alert";
        decision.escalation_required = true;
      } else if (isAfterHours) {
        decision.business_priority = "high";
        decision.recommended_action = "alert";
      } else {
        decision.business_priority = "medium";
        decision.recommended_action = "monitor";
      }
    }

    if (object_type === "car") {
      if (zone === "parking") {
        decision.business_priority = "low";
        decision.recommended_action = "log";
      } else {
        decision.business_priority = "medium";
        decision.recommended_action = "monitor";
      }
    }

    // Apply confidence-based rules
    if (confidence > 0.8) {
      decision.business_priority = this.raisePriority(
        decision.business_priority,
      );
    }

    // Add business context
    decision.business_context = {
      is_business_hours: !isAfterHours,
      zone_risk_level: this.getZoneRiskLevel(zone),
      time_based_modifier: isAfterHours ? 1.5 : 1.0,
      requires_immediate_response: decision.business_priority === "critical",
    };

    return decision;
  }

  raisePriority(currentPriority) {
    const priorities = ["low", "medium", "high", "critical"];
    const currentIndex = priorities.indexOf(currentPriority);
    return priorities[Math.min(currentIndex + 1, priorities.length - 1)];
  }

  getZoneRiskLevel(zone) {
    const riskLevels = {
      entrance: "high",
      restricted: "critical",
      parking: "low",
      perimeter: "medium",
      garden: "low",
    };
    return riskLevels[zone] || "medium";
  }

  processAggregatedEvents(events) {
    const decisions = events.map((event) => this.processDetection(event));

    // Analyze patterns
    const patterns = this.analyzePatterns(decisions);

    return {
      individual_decisions: decisions,
      pattern_analysis: patterns,
      overall_threat_level: this.calculateOverallThreat(decisions),
      recommended_actions: this.recommendActions(decisions, patterns),
    };
  }

  analyzePatterns(decisions) {
    const patterns = {
      repeated_detections: {},
      zone_clustering: {},
      time_clustering: {},
      escalation_pattern: false,
    };

    // Count detections by type and zone
    decisions.forEach((decision) => {
      const key = `${decision.original_detection.object_type}_${decision.original_detection.zone}`;
      patterns.repeated_detections[key] =
        (patterns.repeated_detections[key] || 0) + 1;
    });

    // Check for escalation pattern
    const criticalCount = decisions.filter(
      (d) => d.business_priority === "critical",
    ).length;
    patterns.escalation_pattern = criticalCount > 1;

    return patterns;
  }

  calculateOverallThreat(decisions) {
    const priorityWeights = { low: 1, medium: 2, high: 3, critical: 4 };
    const totalWeight = decisions.reduce((sum, decision) => {
      return sum + priorityWeights[decision.business_priority];
    }, 0);

    const avgWeight = totalWeight / decisions.length;

    if (avgWeight >= 3.5) return "critical";
    if (avgWeight >= 2.5) return "high";
    if (avgWeight >= 1.5) return "medium";
    return "low";
  }

  recommendActions(decisions, patterns) {
    const actions = [];

    if (patterns.escalation_pattern) {
      actions.push({
        type: "escalate_to_security",
        priority: "immediate",
        reason: "Multiple critical events detected",
      });
    }

    const highPriorityCount = decisions.filter((d) =>
      ["high", "critical"].includes(d.business_priority),
    ).length;

    if (highPriorityCount > 2) {
      actions.push({
        type: "notify_management",
        priority: "urgent",
        reason: "Multiple high-priority security events",
      });
    }

    return actions;
  }
}

// Main execution
async function main() {
  const inputFile = process.argv
    .find((arg) => arg.startsWith("--input"))
    ?.split("=")[1];
  const outputFile = process.argv
    .find((arg) => arg.startsWith("--output"))
    ?.split("=")[1];

  if (!inputFile) {
    console.error("Input file required: --input=file.json");
    process.exit(1);
  }

  try {
    const inputData = JSON.parse(fs.readFileSync(inputFile, "utf8"));
    const engine = new BusinessRulesEngine();

    let result;
    if (Array.isArray(inputData.detections)) {
      result = engine.processAggregatedEvents(inputData.detections);
    } else if (inputData.enhanced_detections) {
      result = engine.processAggregatedEvents(inputData.enhanced_detections);
    } else {
      result = engine.processDetection(inputData);
    }

    const output = {
      timestamp: new Date().toISOString(),
      processor: "business_rules_engine",
      input_source: inputData.source || "unknown",
      business_analysis: result,
    };

    if (outputFile) {
      fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
    } else {
      console.log(JSON.stringify(output, null, 2));
    }
  } catch (error) {
    const errorOutput = {
      error: error.message,
      processor: "business_rules_engine",
      success: false,
    };
    console.error(JSON.stringify(errorOutput, null, 2));
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { BusinessRulesEngine };
```

## scripts/sensor_analytics.py

```python
#!/usr/bin/env python3
"""
Python Sensor Analytics Processor
Performs anomaly detection and statistical analysis on sensor data
"""

import sys
import json
import argparse
import os
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import deque
import statistics

class SensorAnalytics:
    def __init__(self):
        self.anomaly_threshold = float(os.getenv('CONFIG_ANOMALY_THRESHOLD', '2.5'))
        self.window_size = int(os.getenv('CONFIG_WINDOW_SIZE', '100'))
        self.min_samples = int(os.getenv('CONFIG_MIN_SAMPLES', '10'))

        # In-memory storage for this session
        self.sensor_history = {}

    def detect_anomalies(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detect anomalies in sensor data using statistical methods"""
        anomalies = []
        processed_sensors = {}

        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            value = reading.get('value', 0)
            timestamp = reading.get('timestamp', datetime.now().isoformat())

            if sensor_id not in self.sensor_history:
                self.sensor_history[sensor_id] = deque(maxlen=self.window_size)

            history = self.sensor_history[sensor_id]
            history.append(value)

            if len(history) >= self.min_samples:
                # Calculate z-score for anomaly detection
                mean_val = statistics.mean(history)
                std_val = statistics.stdev(history) if len(history) > 1 else 0

                if std_val > 0:
                    z_score = abs(value - mean_val) / std_val
                    is_anomaly = z_score > self.anomaly_threshold
                else:
                    is_anomaly = False
                    z_score = 0

                sensor_stats = {
                    'sensor_id': sensor_id,
                    'current_value': value,
                    'mean': mean_val,
                    'std': std_val,
                    'z_score': z_score,
                    'is_anomaly': is_anomaly,
                    'timestamp': timestamp,
                    'sample_count': len(history)
                }

                processed_sensors[sensor_id] = sensor_stats

                if is_anomaly:
                    anomalies.append({
                        'sensor_id': sensor_id,
                        'value': value,
                        'expected_range': [mean_val - 2*std_val, mean_val + 2*std_val],
                        'z_score': z_score,
                        'severity': self.classify_severity(z_score),
                        'timestamp': timestamp
                    })

        return {
            'anomalies': anomalies,
            'sensor_statistics': processed_sensors,
            'total_sensors': len(processed_sensors),
            'anomaly_count': len(anomalies)
        }

    def classify_severity(self, z_score: float) -> str:
        """Classify anomaly severity based on z-score"""
        if z_score > 4:
            return 'critical'
        elif z_score > 3:
            return 'high'
        elif z_score > 2.5:
            return 'medium'
        else:
            return 'low'

    def analyze_trends(self, sensor_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze trends in sensor data"""
        trends = {}

        # Group by sensor
        sensors = {}
        for reading in sensor_data:
            sensor_id = reading.get('sensor_id', 'unknown')
            if sensor_id not in sensors:
                sensors[sensor_id] = []
            sensors[sensor_id].append(reading)

        for sensor_id, readings in sensors.items():
            if len(readings) < 3:
                continue

            values = [r.get('value', 0) for r in readings]
            timestamps = [r.get('timestamp') for r in readings]

            # Simple trend analysis
            if len(values) >= 2:
                trend_direction = 'stable'
                if values[-1] > values[0]:
                    trend_direction = 'increasing'
                elif values[-1] < values[0]:
                    trend_direction = 'decreasing'

                rate_of_change = (values[-1] - values[0]) / len(values)

                trends[sensor_id] = {
                    'direction': trend_direction,
                    'rate_of_change': rate_of_change,
                    'min_value': min(values),
                    'max_value': max(values),
                    'avg_value': statistics.mean(values),
                    'value_range': max(values) - min(values),
                    'sample_count': len(values)
                }

        return trends

    def generate_alerts(self, anomalies: List[Dict[str, Any]], trends: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate alerts based on anomalies and trends"""
        alerts = []

        # Anomaly-based alerts
        for anomaly in anomalies:
            if anomaly['severity'] in ['critical', 'high']:
                alerts.append({
                    'type': 'anomaly_alert',
                    'sensor_id': anomaly['sensor_id'],
                    'message': f"Anomaly detected: {anomaly['sensor_id']} value {anomaly['value']} (z-score: {anomaly['z_score']:.2f})",
                    'severity': anomaly['severity'],
                    'timestamp': anomaly['timestamp'],
                    'recommended_action': 'investigate_sensor' if anomaly['severity'] == 'critical' else 'monitor_closely'
                })

        # Trend-based alerts
        for sensor_id, trend in trends.items():
            if abs(trend['rate_of_change']) > 10:  # Configurable threshold
                alerts.append({
                    'type': 'trend_alert',
                    'sensor_id': sensor_id,
                    'message': f"Rapid change detected: {sensor_id} changing at rate {trend['rate_of_change']:.2f}",
                    'severity': 'medium',
                    'timestamp': datetime.now().isoformat(),
                    'trend_direction': trend['direction'],
                    'recommended_action': 'check_sensor_calibration'
                })

        return alerts

def main():
    parser = argparse.ArgumentParser(description='Sensor Analytics Processor')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        analytics = SensorAnalytics()

        # Extract sensor data from input
        if 'events' in input_data:
            sensor_data = input_data['events']
        elif isinstance(input_data, list):
            sensor_data = input_data
        else:
            sensor_data = [input_data]

        # Perform analysis
        anomaly_results = analytics.detect_anomalies(sensor_data)
        trend_results = analytics.analyze_trends(sensor_data)
        alerts = analytics.generate_alerts(anomaly_results['anomalies'], trend_results)

        output = {
            'timestamp': datetime.now().isoformat(),
            'processor': 'sensor_analytics',
            'input_count': len(sensor_data),
            'anomaly_analysis': anomaly_results,
            'trend_analysis': trend_results,
            'alerts': alerts,
            'summary': {
                'total_anomalies': len(anomaly_results['anomalies']),
                'critical_anomalies': len([a for a in anomaly_results['anomalies'] if a['severity'] == 'critical']),
                'total_alerts': len(alerts),
                'sensors_analyzed': len(trend_results)
            }
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'sensor_analytics',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/grpc_ml_client.py

```python
#!/usr/bin/env python3
"""
gRPC ML Client for remote inference
Connects to ML serving infrastructure
"""

import sys
import json
import argparse
import os
import grpc
import base64
import numpy as np
from typing import Dict, Any, Optional

# Note: In real implementation, you would have generated protobuf files
# For this example, we'll use a simple REST fallback

class GRPCMLClient:
    def __init__(self):
        self.server_address = os.getenv('CONFIG_GRPC_SERVER', 'localhost:50051')
        self.model_name = os.getenv('CONFIG_MODEL_NAME', 'object_detection')
        self.timeout = int(os.getenv('CONFIG_TIMEOUT', '30'))
        self.use_fallback = True  # Use REST fallback for this example

    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Send prediction request to gRPC server"""
        if self.use_fallback:
            return self._rest_fallback(input_data)
        else:
            return self._grpc_predict(input_data)

    def _grpc_predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Real gRPC prediction (requires protobuf definitions)"""
        try:
            # This would be the real gRPC implementation
            # with grpc.insecure_channel(self.server_address) as channel:
            #     stub = PredictionServiceStub(channel)
            #     request = PredictRequest(...)
            #     response = stub.Predict(request, timeout=self.timeout)
            #     return self._parse_response(response)

            # Placeholder for actual gRPC implementation
            return {
                'error': 'gRPC implementation requires protobuf definitions',
                'fallback_used': True
            }

        except Exception as e:
            return {
                'error': f'gRPC prediction failed: {str(e)}',
                'success': False
            }

    def _rest_fallback(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """REST API fallback for ML inference"""
        import requests

        try:
            # Convert gRPC server address to HTTP
            if '://' not in self.server_address:
                url = f"http://{self.server_address}/predict"
            else:
                url = self.server_address.replace('grpc://', 'http://') + '/predict'

            # Prepare payload
            payload = {
                'model_name': self.model_name,
                'inputs': input_data,
                'parameters': {
                    'timeout': self.timeout
                }
            }

            response = requests.post(
                url,
                json=payload,
                timeout=self.timeout,
                headers={'Content-Type': 'application/json'}
            )

            if response.status_code == 200:
                return response.json()
            else:
                return {
                    'error': f'HTTP {response.status_code}: {response.text}',
                    'success': False
                }

        except requests.exceptions.RequestException as e:
            # If real server not available, return mock prediction
            return self._mock_prediction(input_data)
        except Exception as e:
            return {
                'error': f'REST fallback failed: {str(e)}',
                'success': False
            }

    def _mock_prediction(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Mock prediction for testing"""
        return {
            'model_name': self.model_name,
            'predictions': [
                {
                    'class': 'person',
                    'confidence': 0.92,
                    'bbox': [100, 50, 200, 300]
                },
                {
                    'class': 'car',
                    'confidence': 0.78,
                    'bbox': [300, 200, 500, 400]
                }
            ],
            'inference_time_ms': 45.2,
            'model_version': 'v1.0',
            'server_info': {
                'server': 'mock_server',
                'status': 'healthy'
            },
            'mock_used': True
        }

    def batch_predict(self, batch_data: list) -> Dict[str, Any]:
        """Process batch of predictions"""
        results = []
        total_time = 0

        for i, data in enumerate(batch_data):
            result = self.predict(data)
            results.append({
                'batch_index': i,
                'result': result
            })

            # Accumulate inference time
            if 'inference_time_ms' in result:
                total_time += result['inference_time_ms']

        return {
            'batch_results': results,
            'batch_size': len(batch_data),
            'total_inference_time_ms': total_time,
            'average_inference_time_ms': total_time / len(batch_data) if batch_data else 0
        }

def main():
    parser = argparse.ArgumentParser(description='gRPC ML Client')
    parser.add_argument('--input', required=True, help='Input JSON file')
    parser.add_argument('--output', help='Output JSON file (optional)')
    parser.add_argument('--batch', action='store_true', help='Process as batch')

    args = parser.parse_args()

    try:
        with open(args.input, 'r') as f:
            input_data = json.load(f)

        client = GRPCMLClient()

        if args.batch and isinstance(input_data, list):
            result = client.batch_predict(input_data)
        else:
            result = client.predict(input_data)

        output = {
            'timestamp': '2024-01-01T12:00:00Z',
            'processor': 'grpc_ml_client',
            'server': client.server_address,
            'model': client.model_name,
            'ml_inference': result
        }

        if args.output:
            with open(args.output, 'w') as f:
                json.dump(output, f, indent=2)
        else:
            print(json.dumps(output, indent=2))

    except Exception as e:
        error_output = {
            'error': str(e),
            'processor': 'grpc_ml_client',
            'success': False
        }
        print(json.dumps(error_output, indent=2), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## scripts/alerting.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Alerting Engine
 * Handles multi-channel notifications and escalation
 */

const fs = require("fs");
const https = require("https");
const http = require("http");

const config = {
  alertChannels: (process.env.CONFIG_ALERT_CHANNELS || "email,webhook").split(
    ",",
  ),
  escalationRules:
    process.env.CONFIG_ESCALATION_RULES ||
    "immediate:critical,5min:high,1h:medium",
  webhookUrl: process.env.CONFIG_WEBHOOK_URL || "http://localhost:8080/webhook",
  slackWebhook: process.env.CONFIG_SLACK_WEBHOOK || "",
  maxRetries: parseInt(process.env.CONFIG_MAX_RETRIES || "3"),
};

class AlertingEngine {
  constructor() {
    this.escalationRules = this.parseEscalationRules();
    this.alertHistory = new Map();
  }

  parseEscalationRules() {
    const rules = {};
    config.escalationRules.split(",").forEach((rule) => {
      const [time, severity] = rule.split(":");
      rules[severity] = this.parseTimeToSeconds(time);
    });
    return rules;
  }

  parseTimeToSeconds(timeStr) {
    if (timeStr === "immediate") return 0;

    const value = parseInt(timeStr);
    if (timeStr.endsWith("min")) return value * 60;
    if (timeStr.endsWith("h")) return value * 3600;
    if (timeStr.endsWith("s")) return value;
    return value; // assume seconds
  }

  async processAlerts(alertData) {
    const alerts = Array.isArray(alertData.alerts)
      ? alertData.alerts
      : [alertData];
    const results = [];

    for (const alert of alerts) {
      const result = await this.processAlert(alert);
      results.push(result);
    }

    return {
      processed_alerts: results,
      total_alerts: alerts.length,
      successful_deliveries: results.filter((r) => r.success).length,
      failed_deliveries: results.filter((r) => !r.success).length,
    };
  }

  async processAlert(alert) {
    const alertId = this.generateAlertId(alert);
    const severity = alert.severity || "medium";
    const escalationDelay = this.escalationRules[severity] || 300;

    const alertResult = {
      alert_id: alertId,
      severity: severity,
      escalation_delay: escalationDelay,
      channels_used: [],
      success: false,
      delivery_results: [],
    };

    // Immediate processing for critical alerts
    if (severity === "critical" || escalationDelay === 0) {
      return await this.deliverAlert(alert, alertResult);
    }

    // Schedule escalation for non-critical alerts
    this.scheduleEscalation(alert, alertResult, escalationDelay);

    return alertResult;
  }

  async deliverAlert(alert, alertResult) {
    const deliveryPromises = [];

    for (const channel of config.alertChannels) {
      switch (channel.trim()) {
        case "webhook":
          deliveryPromises.push(this.sendWebhook(alert));
          break;
        case "slack":
          if (config.slackWebhook) {
            deliveryPromises.push(this.sendSlack(alert));
          }
          break;
        case "email":
          deliveryPromises.push(this.sendEmail(alert));
          break;
        default:
          console.warn(`Unknown alert channel: ${channel}`);
      }
    }

    try {
      const results = await Promise.allSettled(deliveryPromises);

      results.forEach((result, index) => {
        const channel = config.alertChannels[index];
        alertResult.delivery_results.push({
          channel: channel,
          success: result.status === "fulfilled",
          error: result.status === "rejected" ? result.reason : null,
        });

        if (result.status === "fulfilled") {
          alertResult.channels_used.push(channel);
        }
      });

      alertResult.success = alertResult.channels_used.length > 0;
    } catch (error) {
      alertResult.error = error.message;
    }

    return alertResult;
  }

  async sendWebhook(alert) {
    return new Promise((resolve, reject) => {
      const payload = JSON.stringify({
        alert_type: "camel_router_alert",
        timestamp: new Date().toISOString(),
        severity: alert.severity,
        message: alert.message || "Alert triggered",
        source: alert.source || "camel_router",
        details: alert,
      });

      const url = new URL(config.webhookUrl);
      const options = {
        hostname: url.hostname,
        port: url.port || (url.protocol === "https:" ? 443 : 80),
        path: url.pathname + url.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = url.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ channel: "webhook", status: "success" });
        } else {
          reject(new Error(`Webhook failed with status ${res.statusCode}`));
        }
      });

      req.on("error", (error) => {
        reject(new Error(`Webhook request failed: ${error.message}`));
      });

      req.write(payload);
      req.end();
    });
  }

  async sendSlack(alert) {
    if (!config.slackWebhook) {
      throw new Error("Slack webhook URL not configured");
    }

    const payload = JSON.stringify({
      text: `🚨 Alert: ${alert.message || "Camel Router Alert"}`,
      attachments: [
        {
          color: this.getSeverityColor(alert.severity),
          fields: [
            { title: "Severity", value: alert.severity, short: true },
            {
              title: "Source",
              value: alert.source || "camel_router",
              short: true,
            },
            {
              title: "Timestamp",
              value: new Date().toISOString(),
              short: false,
            },
          ],
        },
      ],
    });

    return this.sendWebhookPayload(config.slackWebhook, payload);
  }

  async sendEmail(alert) {
    // Email sending would require additional setup (SMTP, etc.)
    // For now, we'll log and return success
    console.log(`📧 Email alert would be sent: ${alert.message}`);
    return { channel: "email", status: "simulated" };
  }

  getSeverityColor(severity) {
    const colors = {
      critical: "danger",
      high: "warning",
      medium: "warning",
      low: "good",
    };
    return colors[severity] || "warning";
  }

  generateAlertId(alert) {
    const hash = require("crypto")
      .createHash("md5")
      .update(JSON.stringify(alert) + Date.now())
      .digest("hex");
    return hash.substring(0, 8);
  }

  scheduleEscalation(alert, alertResult, delay) {
    setTimeout(async () => {
      console.log(`⏰ Escalating alert after ${delay}s delay`);
      await this.deliverAlert(alert, alertResult);
    }, delay * 1000);
  }

  async sendWebhookPayload(url, payload) {
    return new Promise((resolve, reject) => {
      const urlObj = new URL(url);
      const options = {
        hostname: urlObj.hostname,
        port: urlObj.port || (urlObj.protocol === "https:" ? 443 : 80),
        path: urlObj.pathname + urlObj.search,
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Content-Length": Buffer.byteLength(payload),
        },
      };

      const client = urlObj.protocol === "https:" ? https : http;
      const req = client.request(options, (res) => {
        if (res.statusCode >= 200 && res.statusCode < 300) {
          resolve({ status: "success" });
        } else {
          reject(new Error(`Request failed with status ${res.statusCode}`));
        }
      });

      req.on("error", reject);
      req.write(payload);
      req.end();
    });
  }
}

// Main execution
async function main() {
  const inputFile = process.argv
    .find((arg) => arg.startsWith("--input"))
    ?.split("=")[1];
  const outputFile = process.argv
    .find((arg) => arg.startsWith("--output"))
    ?.split("=")[1];

  if (!inputFile) {
    console.error("Input file required: --input=file.json");
    process.exit(1);
  }

  try {
    const inputData = JSON.parse(fs.readFileSync(inputFile, "utf8"));
    const engine = new AlertingEngine();

    const result = await engine.processAlerts(inputData);

    const output = {
      timestamp: new Date().toISOString(),
      processor: "alerting_engine",
      configuration: {
        channels: config.alertChannels,
        escalation_rules: config.escalationRules,
      },
      alerting_results: result,
    };

    if (outputFile) {
      fs.writeFileSync(outputFile, JSON.stringify(output, null, 2));
    } else {
      console.log(JSON.stringify(output, null, 2));
    }
  } catch (error) {
    const errorOutput = {
      error: error.message,
      processor: "alerting_engine",
      success: false,
    };
    console.error(JSON.stringify(errorOutput, null, 2));
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { AlertingEngine };
```

## scripts/cpp_processor.cpp

```cpp
/**
 * C++ High-Performance Post-Processor
 * Optimized algorithms for real-time processing
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <cmath>
#include <algorithm>
#include <chrono>
#include <map>
#include <sstream>

// Simple JSON-like structure for this example
struct Detection {
    std::string object_type;
    double confidence;
    std::vector<double> bbox;
    std::string position;
};

struct ProcessingResult {
    std::vector<Detection> optimized_detections;
    double processing_time_ms;
    int original_count;
    int filtered_count;
    std::string algorithm_used;
};

class CPPPostProcessor {
private:
    double nms_threshold;
    double confidence_threshold;
    std::string algorithm;

public:
    CPPPostProcessor() {
        // Load configuration from environment variables
        nms_threshold = std::getenv("CONFIG_NMS_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_NMS_THRESHOLD")) : 0.5;
        confidence_threshold = std::getenv("CONFIG_CONFIDENCE_THRESHOLD") ?
            std::stod(std::getenv("CONFIG_CONFIDENCE_THRESHOLD")) : 0.6;
        algorithm = std::getenv("CONFIG_ALGORITHM") ?
            std::getenv("CONFIG_ALGORITHM") : "fast_nms";
    }

    ProcessingResult processDetections(const std::vector<Detection>& detections) {
        auto start = std::chrono::high_resolution_clock::now();

        std::vector<Detection> filtered = filterByConfidence(detections);
        std::vector<Detection> optimized;

        if (algorithm == "fast_nms") {
            optimized = fastNonMaxSuppression(filtered);
        } else if (algorithm == "sort_confidence") {
            optimized = sortByConfidence(filtered);
        } else {
            optimized = filtered;
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);

        ProcessingResult result;
        result.optimized_detections = optimized;
        result.processing_time_ms = duration.count() / 1000.0;
        result.original_count = detections.size();
        result.filtered_count = optimized.size();
        result.algorithm_used = algorithm;

        return result;
    }

private:
    std::vector<Detection> filterByConfidence(const std::vector<Detection>& detections) {
        std::vector<Detection> filtered;
        for (const auto& det : detections) {
            if (det.confidence >= confidence_threshold) {
                filtered.push_back(det);
            }
        }
        return filtered;
    }

    std::vector<Detection> fastNonMaxSuppression(const std::vector<Detection>& detections) {
        if (detections.empty()) return {};

        std::vector<Detection> sorted_detections = detections;
        std::sort(sorted_detections.begin(), sorted_detections.end(),
                  [](const Detection& a, const Detection& b) {
                      return a.confidence > b.confidence;
                  });

        std::vector<Detection> result;
        std::vector<bool> suppressed(sorted_detections.size(), false);

        for (size_t i = 0; i < sorted_detections.size(); ++i) {
            if (suppressed[i]) continue;

            result.push_back(sorted_detections[i]);

            for (size_t j = i + 1; j < sorted_detections.size(); ++j) {
                if (suppressed[j]) continue;

                double iou = calculateIoU(sorted_detections[i].bbox, sorted_detections[j].bbox);
                if (iou > nms_threshold) {
                    suppressed[j] = true;
                }
            }
        }

        return result;
    }

    std::vector<Detection> sortByConfidence(const std::vector<Detection>& detections) {
        std::vector<Detection> sorted_detections = detections;
        std::sort(sorted_detections.begin(), sorted_detections.end(),
                  [](const Detection& a, const Detection& b) {
                      return a.confidence > b.confidence;
                  });
        return sorted_detections;
    }

    double calculateIoU(const std::vector<double>& box1, const std::vector<double>& box2) {
        if (box1.size() < 4 || box2.size() < 4) return 0.0;

        double x1 = std::max(box1[0], box2[0]);
        double y1 = std::max(box1[1], box2[1]);
        double x2 = std::min(box1[2], box2[2]);
        double y2 = std::min(box1[3], box2[3]);

        if (x2 <= x1 || y2 <= y1) return 0.0;

        double intersection = (x2 - x1) * (y2 - y1);
        double area1 = (box1[2] - box1[0]) * (box1[3] - box1[1]);
        double area2 = (box2[2] - box2[0]) * (box2[3] - box2[1]);
        double union_area = area1 + area2 - intersection;

        return union_area > 0 ? intersection / union_area : 0.0;
    }
};

// Simple JSON parsing (in real implementation, use proper JSON library)
std::vector<Detection> parseDetections(const std::string& json_str) {
    std::vector<Detection> detections;

    // This is a simplified parser - in production use nlohmann/json or similar
    // For demo purposes, create mock detections
    Detection det1;
    det1.object_type = "person";
    det1.confidence = 0.85;
    det1.bbox = {100, 100, 200, 300};
    det1.position = "center";

    Detection det2;
    det2.object_type = "car";
    det2.confidence = 0.92;
    det2.bbox = {300, 150, 450, 280};
    det2.position = "right";

    detections.push_back(det1);
    detections.push_back(det2);

    return detections;
}

std::string formatOutput(const ProcessingResult& result) {
    std::ostringstream oss;
    oss << "{\n";
    oss << "  \"timestamp\": \"2024-01-01T12:00:00Z\",\n";
    oss << "  \"processor\": \"cpp_postprocessor\",\n";
    oss << "  \"algorithm_used\": \"" << result.algorithm_used << "\",\n";
    oss << "  \"processing_time_ms\": " << result.processing_time_ms << ",\n";
    oss << "  \"original_count\": " << result.original_count << ",\n";
    oss << "  \"filtered_count\": " << result.filtered_count << ",\n";
    oss << "  \"optimized_detections\": [\n";

    for (size_t i = 0; i < result.optimized_detections.size(); ++i) {
        const auto& det = result.optimized_detections[i];
        oss << "    {\n";
        oss << "      \"object_type\": \"" << det.object_type << "\",\n";
        oss << "      \"confidence\": " << det.confidence << ",\n";
        oss << "      \"position\": \"" << det.position << "\",\n";
        oss << "      \"bbox\": [" << det.bbox[0] << ", " << det.bbox[1]
            << ", " << det.bbox[2] << ", " << det.bbox[3] << "]\n";
        oss << "    }";
        if (i < result.optimized_detections.size() - 1) oss << ",";
        oss << "\n";
    }

    oss << "  ]\n";
    oss << "}\n";

    return oss.str();
}

int main(int argc, char* argv[]) {
    std::string input_file;
    std::string output_file;

    // Parse command line arguments
    for (int i = 1; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg.find("--input=") == 0) {
            input_file = arg.substr(8);
        } else if (arg.find("--output=") == 0) {
            output_file = arg.substr(9);
        }
    }

    if (input_file.empty()) {
        std::cerr << "Error: Input file required (--input=file.json)" << std::endl;
        return 1;
    }

    try {
        // Read input file
        std::ifstream file(input_file);
        if (!file.is_open()) {
            std::cerr << "Error: Cannot open input file: " << input_file << std::endl;
            return 1;
        }

        std::string json_content((std::istreambuf_iterator<char>(file)),
                                 std::istreambuf_iterator<char>());
        file.close();

        // Parse detections
        std::vector<Detection> detections = parseDetections(json_content);

        // Process with C++ optimizer
        CPPPostProcessor processor;
        ProcessingResult result = processor.processDetections(detections);

        // Format output
        std::string output = formatOutput(result);

        // Write output
        if (!output_file.empty()) {
            std::ofstream outfile(output_file);
            if (outfile.is_open()) {
                outfile << output;
                outfile.close();
            } else {
                std::cerr << "Error: Cannot write to output file: " << output_file << std::endl;
                return 1;
            }
        } else {
            std::cout << output;
        }

    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return 1;
    }

    return 0;
}
```

## scripts/Cargo.toml (Rust Project)

```toml
[package]
name = "camel-processors"
version = "0.1.0"
edition = "2021"

[[bin]]
name = "data_preprocessor"
path = "src/data_preprocessor.rs"

[[bin]]
name = "performance_analyzer"
path = "src/performance_analyzer.rs"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
clap = { version = "4.0", features = ["derive"] }
anyhow = "1.0"
chrono = { version = "0.4", features = ["serde"] }
rayon = "1.7"
ndarray = "0.15"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"
```

## scripts/src/data_preprocessor.rs (Rust)

```rust
//! High-performance data preprocessing in Rust
//! Optimized for SIMD operations and parallel processing

use anyhow::{Context, Result};
use clap::Parser;
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use std::fs;
use std::time::Instant;

#[derive(Parser, Debug)]
#[command(name = "data_preprocessor")]
#[command(about = "High-performance data preprocessing")]
struct Args {
    #[arg(long)]
    input: String,

    #[arg(long)]
    output: Option<String>,
}

#[derive(Debug, Deserialize)]
struct InputData {
    #[serde(default)]
    data: Vec<f64>,
    #[serde(default)]
    batch_data: Vec<Vec<f64>>,
    #[serde(default)]
    config: ProcessingConfig,
}

#[derive(Debug, Deserialize)]
struct ProcessingConfig {
    #[serde(default = "default_batch_size")]
    batch_size: usize,
    #[serde(default = "default_normalize")]
    normalize: bool,
    #[serde(default = "default_parallel")]
    parallel: bool,
    #[serde(default = "default_simd")]
    simd_enabled: bool,
}

fn default_batch_size() -> usize { 32 }
fn default_normalize() -> bool { true }
fn default_parallel() -> bool { true }
fn default_simd() -> bool { true }

impl Default for ProcessingConfig {
    fn default() -> Self {
        Self {
            batch_size: default_batch_size(),
            normalize: default_normalize(),
            parallel: default_parallel(),
            simd_enabled: default_simd(),
        }
    }
}

#[derive(Debug, Serialize)]
struct ProcessingResult {
    timestamp: String,
    processor: String,
    processing_time_ms: f64,
    input_size: usize,
    output_size: usize,
    batches_processed: usize,
    preprocessed_data: Vec<Vec<f64>>,
    statistics: DataStatistics,
}

#[derive(Debug, Serialize)]
struct DataStatistics {
    mean: f64,
    std_dev: f64,
    min: f64,
    max: f64,
    total_elements: usize,
}

struct DataPreprocessor {
    config: ProcessingConfig,
}

impl DataPreprocessor {
    fn new(config: ProcessingConfig) -> Self {
        Self { config }
    }

    fn preprocess(&self, input: InputData) -> Result<ProcessingResult> {
        let start_time = Instant::now();

        // Determine data to process
        let data_to_process = if !input.batch_data.is_empty() {
            input.batch_data
        } else if !input.data.is_empty() {
            vec![input.data]
        } else {
            return Err(anyhow::anyhow!("No data provided"));
        };

        // Process in batches
        let batches: Vec<Vec<Vec<f64>>> = data_to_process
            .chunks(self.config.batch_size)
            .map(|chunk| chunk.to_vec())
            .collect();

        let processed_batches: Vec<Vec<f64>> = if self.config.parallel {
            batches
                .into_par_iter()
                .map(|batch| self.process_batch(batch))
                .collect()
        } else {
            batches
                .into_iter()
                .map(|batch| self.process_batch(batch))
                .collect()
        };

        // Flatten results
        let flattened: Vec<f64> = processed_batches
            .iter()
            .flatten()
            .copied()
            .collect();

        // Calculate statistics
        let stats = self.calculate_statistics(&flattened);

        // Reshape back to batches
        let final_batches: Vec<Vec<f64>> = processed_batches;

        let processing_time = start_time.elapsed().as_secs_f64() * 1000.0;

        Ok(ProcessingResult {
            timestamp: chrono::Utc::now().to_rfc3339(),
            processor: "rust_data_preprocessor".to_string(),
            processing_time_ms: processing_time,
            input_size: data_to_process.len(),
            output_size: final_batches.len(),
            batches_processed: final_batches.len(),
            preprocessed_data: final_batches,
            statistics: stats,
        })
    }

    fn process_batch(&self, batch: Vec<Vec<f64>>) -> Vec<f64> {
        let flattened: Vec<f64> = batch.into_iter().flatten().collect();

        if self.config.normalize {
            self.normalize_data(flattened)
        } else {
            flattened
        }
    }

    fn normalize_data(&self, data: Vec<f64>) -> Vec<f64> {
        if data.is_empty() {
            return data;
        }

        let mean = data.iter().sum::<f64>() / data.len() as f64;
        let variance = data
            .iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / data.len() as f64;
        let std_dev = variance.sqrt();

        if std_dev == 0.0 {
            return data;
        }

        if self.config.simd_enabled {
            // Use SIMD-like operations with rayon
            data.into_par_iter()
                .map(|x| (x - mean) / std_dev)
                .collect()
        } else {
            data.into_iter()
                .map(|x| (x - mean) / std_dev)
                .collect()
        }
    }

    fn calculate_statistics(&self, data: &[f64]) -> DataStatistics {
        if data.is_empty() {
            return DataStatistics {
                mean: 0.0,
                std_dev: 0.0,
                min: 0.0,
                max: 0.0,
                total_elements: 0,
            };
        }

        let mean = data.iter().sum::<f64>() / data.len() as f64;
        let variance = data
            .iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / data.len() as f64;
        let std_dev = variance.sqrt();
        let min = data.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max = data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));

        DataStatistics {
            mean,
            std_dev,
            min,
            max,
            total_elements: data.len(),
        }
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    // Read input file
    let input_content = fs::read_to_string(&args.input)
        .with_context(|| format!("Failed to read input file: {}", args.input))?;

    // Parse input JSON
    let input_data: InputData = serde_json::from_str(&input_content)
        .with_context(|| "Failed to parse input JSON")?;

    // Load configuration from environment if available
    let mut config = input_data.config;
    if let Ok(batch_size) = std::env::var("CONFIG_BATCH_SIZE") {
        config.batch_size = batch_size.parse().unwrap_or(config.batch_size);
    }
    if let Ok(normalize) = std::env::var("CONFIG_NORMALIZE") {
        config.normalize = normalize.parse().unwrap_or(config.normalize);
    }

    // Process data
    let preprocessor = DataPreprocessor::new(config);
    let result = preprocessor.preprocess(input_data)?;

    // Output results
    let output_json = serde_json::to_string_pretty(&result)
        .with_context(|| "Failed to serialize output")?;

    if let Some(output_file) = args.output {
        fs::write(&output_file, output_json)
            .with_context(|| format!("Failed to write output file: {}", output_file))?;
    } else {
        println!("{}", output_json);
    }

    Ok(())
}
```

## scripts/go.mod

```go
module camel-processors

go 1.21

require (
    github.com/golang/protobuf v1.5.3
    google.golang.org/grpc v1.57.0
    google.golang.org/protobuf v1.31.0
)

require (
    golang.org/x/net v0.12.0 // indirect
    golang.org/x/sys v0.10.0 // indirect
    golang.org/x/text v0.11.0 // indirect
    google.golang.org/genproto/googleapis/rpc v0.0.0-20230726155614-23370e0ffb3e // indirect
)
```

## scripts/package.json

```json
{
  "name": "camel-router-processors",
  "version": "1.0.0",
  "description": "Node.js processors for Camel Router",
  "main": "business_rules.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "lint": "eslint *.js",
    "format": "prettier --write *.js"
  },
  "keywords": ["camel-router", "processing", "business-rules", "alerting"],
  "author": "Camel Router Team",
  "license": "MIT",
  "dependencies": {
    "axios": "^1.5.0",
    "lodash": "^4.17.21",
    "moment": "^2.29.4"
  },
  "devDependencies": {
    "eslint": "^8.47.0",
    "prettier": "^3.0.1"
  },
  "engines": {
    "node": ">=16.0.0"
  }
}
```

# Missing External Scripts

## scripts/business_rules.js

```javascript
#!/usr/bin/env node
/**
 * Node.js Business Rules Processor
 * Handles complex business logic and decision making
 */

const fs = require('fs');
const path = require('path');

// Configuration from environment variables
const config = {
    rulesFile: process.env.CONFIG_RULES_FILE || 'security_rules.json',
    alertThreshold: parseFloat(process.env.CONFIG_ALERT_THRESHOLD || '0.7'),
    businessHours: process.env.CONFIG_BUSINESS_HOURS || '09:00-17:00',
    escalationTime: parseInt(process.env.CONFIG_ESCALATION_TIME || '300') // seconds
};

class BusinessRulesEngine {
    constructor() {
        this.rules = this.loadRules();
    }

    loadRules() {
        try {
            if (fs.existsSync(config.rulesFile)) {
                return JSON.parse(fs.readFileSync(config.rulesFile, 'utf8'));
            }
        } catch (error) {
            console.error(`Error loading rules: ${error.message}`);
        }

        // Default rules
        return {
            person_detection: {
                business_hours: { priority: 'medium', action: 'log' },
                after_hours: { priority: 'high', action: 'alert' },
                restricted_zones: { priority: 'critical', action: 'immediate_alert' }
            },
            vehicle_detection: {
                parking_area: { priority: 'low', action: 'log' },
                restricted_area: { priority: 'high', action: 'alert' }
            },
            threat_assessment: {
                high_confidence: { threshold: 0.8, action: 'escalate' },
                medium_confidence: { threshold: 0.6, action: 'monitor' },
                low_confidence: { threshold: 0.3, action: 'log' }
            }
        };
    }

    isBusinessHours() {
        const now = new Date();
        const currentTime = now.getHours() * 100 + now.getMinutes();
        const [start, end] = config.businessHours.split('-').map(time => {
            const [hours, minutes] = time.split(':').map(Number);
            return hours * 100 + minutes;
        });
        return currentTime >= start && currentTime <= end;
    }

    processDetection(detection) {
        const { object_type, confidence, zone, position } = detection;
        const isAfterHours = !this.isBusinessHours();

        let decision = {
            original_detection: detection,
            business_priority: 'low',
            recommended_action: 'log',
            escalation_required: false,
            business_context: {},
            timestamp: new Date().toISOString()
        };

        // Apply object-specific rules
        if (object_type === 'person') {
            if (zone === 'restricted' || zone === 'entrance') {
                decision.business_priority = 'critical';
                decision.recommended_action = 'immediate_alert';
                decision.escalation_required = true;
            } else if (isAfterHours) {
                decision.business_priority = 'high';
                decision.recommended_action = 'alert';
            } else {
                decision.business_priority = 'medium';
                decision.recommended_action = 'monitor';
            }
        }

        if (object_type === 'car') {
            if (zone === 'parking') {
                decision.business_priority = 'low';
                decision.recommended_action = 'log';
            } else {
                decision.business_priority = 'medium';
                decision.recommended_action = 'monitor';
            }
        }

        // Apply confidence-based rules
        if (confidence > 0.8) {
            decision.business_priority = this.raisePriority(decision.business_priority);
        }

        // Add business context
        decision.business_context = {
            is_business_hours: !isAfterHours,
            zone_risk_level: this.getZoneRiskLevel(zone),
            time_based_modifier: isAfterHours ? 1.5 : 1.0,
            requires_immediate_response: decision.business_priority === 'critical'
        };

        return decision;
    }

    raisePriority(currentPriority) {
        const priorities = ['low', 'medium', 'high', 'critical'];
        const currentIndex = priorities.indexOf(currentPriority);
        return priorities[Math.min(currentIndex + 1, priorities.length - 1)];
    }

    getZoneRiskLevel(zone) {
        const riskLevels = {
            'entrance': 'high',
            'restricted': 'critical',
            'parking': 'low',
            'perimeter': 'medium',
            'garden': 'low'
        };
        return riskLevels[zone] || 'medium';
    }

    processAggregatedEvents(events) {
        const decisions = events.map(event => this.processDetection(event));

        // Analyze patterns
        const patterns = this.analyzePatterns(decisions);

        return {
            individual_decisions: decisions,
            pattern_analysis: patterns,
            overall_threat_level: this.calculateOverallThreat(decisions),
            recommended_actions: this.recommendActions(decisions, patterns)
        };
    }

    analyzePatterns(decisions) {
        const patterns = {
            repeated_detections: {},
            zone_clustering: {},
            time_clustering: {},
            escalation_pattern: false
        };

        // Count detections by type and zone
        decisions.forEach(decision => {
            const key = `${decision.original_detection.object_type}_${decision.original_detection.zone}`;
            patterns.repeated_detections[key] = (patterns.repeated_detections[key] || 0) + 1;
        });

        // Check for escalation pattern
        const criticalCount = decisions.filter(d => d.business_priority === 'critical').length;
        patterns.escalation_pattern = criticalCount > 1;

        return patterns;
    }

    calculateOverallThreat(decisions) {
        const priorityWeights = { low: 1, medium: 2, high: 3, critical: 4 };
        const totalWeight = decisions.reduce((sum, decision) => {
            return sum + priorityWeights[decision.business_priority];
        }, 0);

        const avgWeight = totalWeight / decisions.length;

        if (avgWeight >= 3.5) return 'critical';
        if (avgWeight >= 2.5) return 'high';
        if (avgWeight
```
